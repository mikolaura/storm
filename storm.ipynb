{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8369ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_topic = \"Impact of millon-plus token context window language models on RAG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f12a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1fa9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelatedSubject(topics=['Large language models', 'Retrieval augmented generation', 'Context window', 'Natural language processing', 'Artificial intelligence', 'Machine learning', 'Deep learning', 'Transformer networks', 'Wikipedia'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "fast_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.0,)\n",
    "long_context_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "    temperature=0.0)\n",
    "\n",
    "gen_related_topics_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"I'm writing a Wikipedia page for a topic mentionde below. Please identify and recomend some Wikipedia pagens on clasely related topics\n",
    "    \n",
    "    Please list the as many subject and urls as you can\n",
    "    \n",
    "    Topic of interest: {topic}\"\"\"\n",
    ")\n",
    "\n",
    "class RelatedSubject(BaseModel):\n",
    "    topics: List[str] = Field(\n",
    "    description=\"Comprehensive list of related subjects as background research\",\n",
    "    )\n",
    "\n",
    "expand_chain = gen_related_topics_prompt | fast_llm.with_structured_output(RelatedSubject)\n",
    "\n",
    "related_subjects = await expand_chain.ainvoke({'topic': example_topic})\n",
    "related_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f317fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3988\\833876501.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  perspectives.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'editors': [{'affiliation': 'Independent AI Researcher',\n",
       "   'name': 'Dr. Anya Sharma',\n",
       "   'role': 'Technical Expert',\n",
       "   'description': 'Focuses on the technical aspects of RAG, including indexing strategies, retrieval algorithms, and prompt engineering techniques optimized for large context windows.'},\n",
       "  {'affiliation': 'Enterprise Solutions Architect',\n",
       "   'name': 'Ben Carter',\n",
       "   'role': 'Industry Application',\n",
       "   'description': 'Concerned with the practical applications of large context RAG in enterprise settings, focusing on scalability, security, and integration with existing systems.'},\n",
       "  {'affiliation': 'AI Ethics Advocate',\n",
       "   'name': 'Dr. Sofia Rodriguez',\n",
       "   'role': 'Ethics and Bias',\n",
       "   'description': 'Examines the ethical implications of using large context RAG, including potential biases in retrieved information and the impact on transparency and accountability.'},\n",
       "  {'affiliation': 'Computational Linguist',\n",
       "   'name': 'Dr. Kenji Tanaka',\n",
       "   'role': 'Linguistic Analysis',\n",
       "   'description': 'Investigates the impact of large context windows on the linguistic properties of generated text, such as coherence, fluency, and style.'},\n",
       "  {'affiliation': 'Information Retrieval Specialist',\n",
       "   'name': 'Lei Chen',\n",
       "   'role': 'Information Retrieval',\n",
       "   'description': 'Focuses on optimizing information retrieval strategies within large context RAG systems, considering factors like query formulation, document ranking, and relevance filtering.'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.runnables import RunnableLambda, chain as as_runnable\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)\n",
    "\n",
    "def format_doc(doc, max_length=1000):\n",
    "    related = \"- \".join(doc.metadata['categories'])\n",
    "    return f\"### {doc.metadata['title']}\\n\\nSummary: {doc.page_content}\\nRelated: {related}\\n\\n\"[:max_length]\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([format_doc(doc) for doc in docs])\n",
    "\n",
    "\n",
    "class Editor(BaseModel):\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the editor\"\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"Name of the editor\",\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"Role of the editor in the context of the topic.\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Description of the editor's focus, concers, and motives`\"\n",
    "    )\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\"\n",
    "    \n",
    "class Perspectives(BaseModel):\n",
    "    editors: List[Editor] = Field(\n",
    "        description=\"List of editors with their perspectives on the topic\"\n",
    "    )\n",
    "\n",
    "gen_perspectives_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "         \"\"\"You need to select a diverse(and distinc) group of Wikipedia editors who will work together to create a comprehensive article on the topic.\n",
    "         You can use other Wikipedia pages of related topics for inspiration. For each editor, add description of what they will focus on.\n",
    "         \n",
    "         Wiki page outlines of related topics for inspiration: \n",
    "         {examples}\"\"\"),\n",
    "         (\"user\",\"Topic of interest: {topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_perspectives_chain = gen_perspectives_prompt | ChatGoogleGenerativeAI(model='gemini-2.0-flash').with_structured_output(Perspectives)\n",
    "\n",
    "@as_runnable\n",
    "async def survey_subjects(topics: str):\n",
    "    reletaed_subjects = await expand_chain.ainvoke({'topic': topics})\n",
    "    retrieved_docs = await wikipedia_retriever.abatch(reletaed_subjects.topics, return_exceptions=True)\n",
    "    all_docs = []\n",
    "    for docs in retrieved_docs:\n",
    "        if isinstance(docs, BaseException):\n",
    "            continue\n",
    "        all_docs.extend(docs)\n",
    "    formatted = format_docs(all_docs)\n",
    "    return await gen_perspectives_chain.ainvoke({\n",
    "        \"examples\": formatted,\n",
    "        \"topic\": topics\n",
    "    })\n",
    "\n",
    "perspectives = await survey_subjects.ainvoke(example_topic)\n",
    "perspectives.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adee1233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affiliation='Independent AI Researcher' name='Dr. Anya Sharma' role='Technical Expert' description='Focuses on the technical aspects of RAG, including indexing strategies, retrieval algorithms, and prompt engineering techniques optimized for large context windows.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Dr. Sharma, it's a pleasure to speak with you.  My name is Alex, and I'm a Wikipedia editor working on an article about the impact of million-plus token context window language models on Retrieval Augmented Generation (RAG).  My focus is on the practical implications and limitations of these models for real-world applications.  My first question is: what are some of the most significant challenges you've encountered in indexing and retrieving information for these extremely long context windows, beyond the obvious computational constraints?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage, AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "from typing import Annotated, Sequence\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "def add_messages(left, right):\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    return left+right\n",
    "\n",
    "def update_references(references, new_references):\n",
    "    if not references:\n",
    "        references = {}\n",
    "    references.update(new_references)\n",
    "    return references\n",
    "\n",
    "def update_editor(editor, new_editor):\n",
    "    # Can only set at the outset\n",
    "    if not editor:\n",
    "        return new_editor\n",
    "    return editor\n",
    "\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    references: Annotated[Optional[dict], update_references]\n",
    "    editor: Annotated[Optional[Editor], update_editor]\n",
    "\n",
    "\n",
    "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experienced Wikipedia writer and want to edit a specific page. \\\n",
    "Besides your identity as a Wikipedia writer, you have a specific focus when researching the topic. \\\n",
    "Now, you are chatting with an expert to get information. Ask good questions to get more useful information.\n",
    "\n",
    "When you have no more questions to ask, say \"Thank you so much for your help!\" to end the conversation.\\\n",
    "Please only ask one question at a time and don't ask what you have asked before.\\\n",
    "Your questions should be related to the topic you want to write.\n",
    "Be comprehensive and curious, gaining as much unique insight from the expert as possible.\\\n",
    "\n",
    "Stay true to your specific perspective:\n",
    "\n",
    "{persona}\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def tag_with_name(ai_message: AIMessage, name:str):\n",
    "    ai_message.name = name\n",
    "    return ai_message\n",
    "\n",
    "\n",
    "def swap_roles(state: InterviewState, name):\n",
    "    converted = []\n",
    "    for message in state['messages']:\n",
    "        if isinstance(message, AIMessage) and message.name != name:\n",
    "            message = HumanMessage(**message.model_dump(exclude=(\"type\")))\n",
    "        converted.append(message)\n",
    "    return {'messages': converted}\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def generate_question(state:InterviewState):\n",
    "    editor = state['editor']\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=editor.name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | fast_llm\n",
    "        | RunnableLambda(tag_with_name).bind(name=editor.name)\n",
    "    )\n",
    "    result = await gn_chain.ainvoke(state)\n",
    "    return {\"messages\": [result]}\n",
    "\n",
    "\n",
    "print(perspectives.editors[0])\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(f\"So you said you were wrting an article on {example_topic}\")\n",
    "]\n",
    "\n",
    "question = await generate_question.ainvoke(\n",
    "    {'editor':perspectives.editors[0],\n",
    "     \"messages\":messages}\n",
    ")\n",
    "\n",
    "question['messages'][0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85762a1",
   "metadata": {},
   "source": [
    "## Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15da617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: ['challenges indexing and retrieving information million-plus token context window language models', 'limitations of million-plus token context window language models for real-world applications', 'practical implications of million-plus token context window language models for Retrieval Augmented Generation (RAG)']\n"
     ]
    }
   ],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(\n",
    "        description=\"Comprehensive list of search engine queries to answer the user's questons.\"\n",
    "\n",
    "    )\n",
    "\n",
    "gen_queries_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            \"You are a helpful research assistant. Query the search engine to answer the user's questions.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True)\n",
    "    ]\n",
    ")\n",
    "gen_queries_chain = gen_queries_prompt | ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\").with_structured_output(Queries, include_raw=True)\n",
    "\n",
    "queries = await gen_queries_chain.ainvoke({\n",
    "    'messages': [HumanMessage(content=question['messages'][0].content)]\n",
    "})\n",
    "print(f\"Queries: {queries['parsed'].queries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697cb418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3988\\2637305348.py:36: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The most significant challenges in indexing and retrieving information for million-plus token context window language models, beyond computational constraints, include the difficulty of maintaining focus and identifying key information within the vast context, a phenomenon sometimes referred to as the “lost in the middle” problem.  Furthermore, model performance can degrade as the context window approaches its limits, even before the theoretical maximum is reached.  This degradation is often more pronounced than expected, and the effective utilization of all the information within the massive context window remains an open challenge.  Finally, the increased complexity of managing and processing such large contexts introduces new challenges in ensuring accuracy and efficiency, requiring innovative techniques to optimize resource usage and reduce latency.\\n\\nCitations:\\n\\n[1]: https://aiagentslist.com/blog/is-rag-still-relevant-with-million-tokens-llms\\n[2]: https://blog.ayraa.io/rag-is-dead-long-live-rag/\\n[3]: https://medium.com/@adnanmasood/long-context-windows-in-large-language-models-applications-in-comprehension-and-code-03bf4027066f'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "class AnswerWithCitations(BaseModel):\n",
    "    answer: str = Field(\n",
    "        description=\"Comprehensive answer to the user's question with citations.\"\n",
    "    )\n",
    "    cited_urls:List[str] = Field(\n",
    "        description=\"List of urls cited in the answer\"\n",
    "    )\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"{self.answer}\\n\\nCitations:\\n\\n\" + \"\\n\".join(f\"[{i+1}]: {url}\" for i, url in enumerate(self.cited_urls))\n",
    "    \n",
    "\n",
    "gen_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert who can use information effectively. You are chatting with a Wikipedia writer who wants\\\n",
    " to write a Wikipedia page on the topic you know. You have gathered the related information and will now use the information to form a response.\n",
    "\n",
    "Make your response as informative as possible and make sure every sentence is supported by the gathered information.\n",
    "Each response must be backed up by a citation from a reliable source, formatted as a footnote, reproducing the URLS after your response.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_answer_chain = gen_answer_prompt | fast_llm.with_structured_output(\n",
    "    AnswerWithCitations, include_raw=True\n",
    ").with_config(rum_name=\"GenerateAnswer\")\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=4)\n",
    "\n",
    "@tool\n",
    "async def search_engine(query:str):\n",
    "    \"\"\"Search engine to the internet\"\"\"\n",
    "    results = tavily_search.invoke(query)\n",
    "    return [{'content': r['content'], \"url\": r[\"url\"]} for r in results]\n",
    "\n",
    "\n",
    "async def gen_answer(\n",
    "        state: InterviewState,\n",
    "        config: Optional[RunnableConfig] =None,\n",
    "        name: str = \"Subject Matter Expert\",\n",
    "        max_str_len: int = 15000\n",
    "):\n",
    "    \n",
    "    swapped_state = swap_roles(state, name)\n",
    "    queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "    query_result = await search_engine.abatch(\n",
    "        queries['parsed'].queries, config, return_exceptions=True\n",
    "    )\n",
    "    successfull_result = [\n",
    "        res for res in query_result if not isinstance(res, Exception)\n",
    "        ]\n",
    "    all_query_results = {\n",
    "        res['url']: res['content'] for results in successfull_result for res in results\n",
    "    }\n",
    "    dumped = json.dumps(all_query_results)[:max_str_len]\n",
    "    ai_message: AIMessage = queries['raw']\n",
    "    tool_call = queries['raw'].tool_calls[0]\n",
    "    tool_id = tool_call['id']\n",
    "    tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "    swapped_state['messages'].extend([ai_message, tool_message])\n",
    "\n",
    "    generated = await gen_answer_chain.ainvoke(swapped_state)\n",
    "    cited_urls = set(generated['parsed'].cited_urls)\n",
    "    cited_references = {k:v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    formatted_message = AIMessage(name=name, content=generated['parsed'].as_str)\n",
    "    return {\"messages\": [formatted_message], \"references\": cited_references}\n",
    "\n",
    "\n",
    "example_answer = await gen_answer(\n",
    "    {\"messages\": [HumanMessage(content=question['messages'][0].content)]},\n",
    ")\n",
    "\n",
    "\n",
    "example_answer['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "223b2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_turns = 5\n",
    "\n",
    "\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"Subject_Matter_Expert\"):\n",
    "    messages = state[\"messages\"]\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "    if num_responses >= max_num_turns:\n",
    "        return END\n",
    "    last_question = messages[-2]\n",
    "    if last_question.content.endswith(\"Thank you so much for your help!\"):\n",
    "        return END\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "builder = StateGraph(InterviewState)\n",
    "\n",
    "builder.add_node(\"ask_question\", generate_question)\n",
    "builder.add_node(\"answer_question\", gen_answer)\n",
    "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
    "builder.add_edge(\"ask_question\", \"answer_question\")\n",
    "\n",
    "builder.set_entry_point(\"ask_question\")\n",
    "interview_graph = builder.compile().with_config(run_name=\"Conduct Interviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37fbdd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  {'ask_question': {'messages': [AIMessage(content=\"Hello Dr. Sharma, it's a pleasure to speak with you.  My Wikipedia article focuses on the impact of million-plus token context window language models on Retrieval Augmented Generation (RAG) systems.  My first question is: what are the most significant challenges currently faced in indexing and retrieving information for these models, given their vastly expanded context windows?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, name='Dr. Anya Sharma', id='run--f54badac-099e-40ff-aa4d-80f8603d4e9d-0', usage_metadata={'input_tokens': 204, 'output_tokens': 72, 'total_tokens': 276, 'input_token_details': {'cache_read': 0}})]}}\n",
      "--  {'answer_question': {'messages': [AIMessage(content='The most significant challenges in indexing and retrieving information for million-plus token context window language models in RAG systems revolve around several key issues.  First, even with expanded context windows, models often struggle to utilize all the provided information effectively, leading to a phenomenon known as the \"context cliff.\"  Performance degrades significantly as the context window approaches its limit, and models may not fully leverage the available data, even if it\\'s technically within the window.<sup>1</sup>  Second, the sheer volume of data necessitates intelligent filtering and retrieval strategies.  Simply providing a massive amount of data to the model is often less effective than providing a smaller, highly relevant subset.<sup>1</sup>  Third, knowledge bases tend to grow faster than context window sizes, meaning that even with larger windows, the challenge of selecting the most pertinent information remains.<sup>1</sup>  Fourth, data quality is paramount.  Poorly formatted, incorrect, or redundant information in the knowledge base can lead to retrieval of misleading or irrelevant chunks, negatively impacting the LLM\\'s responses.<sup>2</sup>  Finally, issues like chunking errors, poor embedding quality, and missing context can further reduce retrieval accuracy.<sup>2</sup>\\n\\nCitations:\\n\\n[1]: https://blog.ayraa.io/rag-is-dead-long-live-rag/\\n[2]: https://labelstud.io/blog/rag-fundamentals-challenges-and-advanced-techniques/', additional_kwargs={}, response_metadata={}, name='Subject Matter Expert')], 'references': {'https://labelstud.io/blog/rag-fundamentals-challenges-and-advanced-techniques/': 'While RAG improves LLM reliability, real-world applications often expose its limitations. Retrieval can still return irrelevant, incomplete, or even incorrect information. Additionally, issues like chunking errors, poor embedding quality, and missing context can impact response accuracy. To build a high-quality system, we need advanced techniques that refine how data is processed, retrieved, and used. Here we’ll list out a few of the techniques used to alleviate these pain points. [...] Beyond retrieval techniques, **data quality is one of the most critical factors in RAG performance**. If the knowledge base contains poorly formatted, incorrect, or redundant information, retrieval can surface misleading or irrelevant chunks, negatively affecting the LLM’s responses. Issues such as bad chunking strategies, ambiguous wording, and improper segmentation can distort embeddings and reduce retrieval accuracy. Addressing this usually centers on preprocessing techniques like better [...] Conclusion\\n----------\\n\\nRAG is a powerful technique for enhancing LLM performance, but as we’ve explored, implementing it effectively requires ongoing refinement. From ranking and retrieval optimization to handling complex queries and multimodal data, each challenge highlights the need for continuous improvement in how we manage and retrieve information.', 'https://blog.ayraa.io/rag-is-dead-long-live-rag/': 'Another overlooked issue is what we might call the **context cliff** – the way model performance degrades as you approach those lofty context limits. Just because an LLM can take in millions of tokens *format-wise* doesn’t mean it can use all that information effectively. In fact, **research shows that models struggle long before they hit the theoretical max**. A recent benchmark by the NoLiMa Study by Cornell University (1) designed to truly test long-context reasoning (beyond trivial keyword [...] In real deployments, this means that giving an LLM *everything plus the kitchen sink* often works worse than giving it a well-chosen summary or snippet. Practitioners have noticed that for most tasks, a smaller context with highly relevant info beats a huge context of raw data. Retrieval isn’t just a clever trick to overcome old 4K token limits – it’s a way of avoiding overwhelming the model with irrelevant text. Even the latest long-context models **“still fail to utilize information from the [...] It’s the age-old story: as memory grows, so does data. No matter how large context windows get, **knowledge bases will likely grow faster** (just as our storage drives always outpace our RAM). That means you’ll always face a filtering problem. Even if you *could* indiscriminately dump a huge trove of data into the model, you would be showing it only a slice of what you have. Unless that slice is intelligently chosen, you risk omitting what’s important.'}}}\n",
      "--  {'ask_question': {'messages': [AIMessage(content=\"That's extremely helpful, thank you.  My next question is:  what innovative indexing strategies are showing the most promise in handling the scale and complexity of data required for these large context window models, and what are their relative strengths and weaknesses?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, name='Dr. Anya Sharma', id='run--67025219-23ca-47df-a75b-6add4a16bfd6-0', usage_metadata={'input_tokens': 568, 'output_tokens': 51, 'total_tokens': 619, 'input_token_details': {'cache_read': 0}})]}}\n",
      "--  {'answer_question': {'messages': [AIMessage(content=\"Several innovative indexing strategies are showing promise for handling the scale and complexity of data required by million-plus token context window language models.  Hierarchical indexing, for example, organizes information into a multi-level structure, allowing for efficient retrieval of relevant information by progressively narrowing the search space.<sup>1, 2, 3, 4</sup> This approach improves accuracy and scalability, particularly for complex queries and large datasets.<sup>5</sup>  However, building and maintaining these hierarchies can be complex, and the effectiveness depends heavily on the quality of the initial organization.<sup>6</sup>  Graph-based indexing offers another promising approach, leveraging the power of knowledge graphs to capture relationships between entities, even if they don't co-occur in the same document.<sup>7, 8, 9</sup> This allows for more nuanced retrieval and multi-hop reasoning, uncovering insights that linear methods often miss.<sup>10</sup>  However, scalability can be a concern with very large graphs, and the complexity of graph construction and traversal can be significant.<sup>11</sup>  Finally, vector databases, combined with techniques like HNSW and IVF, offer efficient search capabilities for large datasets.<sup>12</sup>  These methods focus on finding the nearest neighbors based on vector similarity, enabling rapid retrieval of relevant information.<sup>12</sup>  However, the effectiveness depends on the quality of the embeddings and the choice of distance metric.<sup>12</sup>  Each approach has its strengths and weaknesses, and the optimal choice often depends on the specific application and dataset.\\n\\nCitations:\\n\\n[1]: https://medium.com/@nirdiamant21/hierarchical-indices-enhancing-rag-systems-43c06330c085\\n[2]: https://medium.com/@nay1228/document-hierarchy-in-rag-boosting-ai-retrieval-efficiency-aa23f21b5fb9\\n[3]: https://pixion.co/blog/rag-strategies-hierarchical-index-retrieval\\n[4]: https://www.reddit.com/r/LangChain/comments/1f6e99e/hierarchical_indices_optimizing_rag_systems_for/\\n[5]: https://github.com/0xZee/Graph-RAG\\n[6]: https://www.chitika.com/graph-based-retrieval-augmented-generation/\\n[7]: https://www.deepset.ai/blog/graph-rag\\n[8]: https://www.elastic.co/search-labs/blog/rag-graph-traversal\\n[9]: https://dataforest.ai/blog/vector-db-for-rag-information-retrieval-with-semantic-search\\n[10]: https://blog.ayraa.io/rag-is-dead-long-live-rag/\\n[11]: https://labelstud.io/blog/rag-fundamentals-challenges-and-advanced-techniques/\\n[12]: https://dataforest.ai/blog/vector-db-for-rag-information-retrieval-with-semantic-search\", additional_kwargs={}, response_metadata={}, name='Subject Matter Expert')], 'references': {'https://medium.com/@nirdiamant21/hierarchical-indices-enhancing-rag-systems-43c06330c085': 'If you’re looking to implement hierarchical indices in your RAG system, here’s a simplified approach:\\n\\n1. **Data Preparation**:   \\n — Clean and organize your data  \\n — Identify logical divisions or create them based on content\\n\\n2. **Building the Hierarchy**:  \\n — Generate top-level summaries  \\n — Create mid-level overviews  \\n — Prepare detailed chunks\\n\\n3. **Indexing**:  \\n — Use vector embeddings for each hierarchical level  \\n — Ensure proper connections between levels [...] Hierarchical indices are significantly improving RAG systems by providing a smarter way to organize and access information. They enable AI systems to better understand context, handle complexity, and deliver more accurate and relevant insights.\\n\\nIf you’re interested in implementing these techniques, check out my RAG techniques repository at <https://github.com/NirDiamant/RAG_Techniques> for practical examples and advanced approaches.\\n\\n— - [...] Retrieval-augmented generation has become popular for good reason. AI systems can answer questions by combining information retrieval with language generation. However, traditional RAG systems can struggle as data becomes more complex and queries more intricate. This is where hierarchical indices come in.\\n\\n# What Are Hierarchical Indices?\\n\\nHierarchical indices are a way of organizing information in a multi-level structure. Here’s a basic breakdown of a 3-tier level structure:', 'https://medium.com/@nay1228/document-hierarchy-in-rag-boosting-ai-retrieval-efficiency-aa23f21b5fb9': '## Working Principles of Document Hierarchy in RAG\\n\\nThe hierarchical structures enhances RAG’s ability to retrieve and generate relevant information.\\n\\n## Case Study: Samsung SDS’s SKE-GPT Implementation\\n\\nSamsung SDS developed SKE-GPT as an enterprise-level RAG system for querying corporate knowledge bases efficiently. The system employed hierarchical indexing to improve retrieval accuracy and contextual understanding while scaling across large datasets.\\n\\n**Implementation Highlights:** [...] Document hierarchy involves organizing into multi-layered structure that reflects logical relationships between different segments of information.\\n\\n**Key Components include:**\\n\\n## Architecture of Document Hierarchy in RAG\\n\\nThe architecture of a hierarchical RAG system is designed to balance efficiency and scalability.\\n\\n**Key elements include:**\\n\\n**1. Hierarchical Indexing**\\n\\n→ Documents are indexed into layers (e.g topics → sections → paragraphs). [...] **3. Hierarchical Index Construction:**\\n\\n→ A tree-like index is built where parent nodes represent high-level structures and child nodes represent finer-grained content.\\n\\n→ Metadata is associated with each node for efficient filtering during retrieval.\\n\\n**4. Dynamic Query Routing:**\\n\\n→ Queries are routed through the hierarchy based on relevant scores calculated using vector similarity metrics (e.g cosine similarity)\\n\\n→ Filters such as metadata constrains refine the search process.', 'https://pixion.co/blog/rag-strategies-hierarchical-index-retrieval': \"While the hierarchical index is a good introduction to leveraging LLMs to improve RAG performance, it's just the\\xa0**tip of the LLM iceberg**. More on how to use LLMs will be discussed in the following article about augmenting user queries and chunks themselves. [...] In the face of growing data volumes, hierarchical index retrieval stands as a robust strategy to\\xa0**improve data accuracy and scalability**. This strategy, which organizes data into a\\xa0**hierarchy**\\xa0and\\xa0**progressively narrows down**\\xa0to the most relevant data, is versatile and adaptable to various use cases, from\\xa0**structured to unstructured data**. [...] One common implementation variant begins with the\\xa0**summary of an entire document**\\xa0as the first level of the hierarchy. That allows us to proceed with the retrieval process by focusing only on the relevant documents,\\xa0**significantly narrowing the search space**. Following this, we can continue the retrieval using any strategy that fits our needs.\", 'https://www.reddit.com/r/LangChain/comments/1f6e99e/hierarchical_indices_optimizing_rag_systems_for/': 'A comprehensive guide on implementing hierarchical indices in RAG systems. This technique significantly improves handling of complex queries and large datasets.', 'https://github.com/0xZee/Graph-RAG': '[](https://github.com/0xZee/Graph-RAG#-graphrag)\\n\\nis a modular graph-based Retrieval-Augmented Generation (RAG) system designed to extract meaningful, structured data from unstructured text using the power of large language models (LLMs). It enhances LLMs’ ability to reason about private data by leveraging knowledge graph memory structures. The system is part of a data pipeline and transformation suite that aims to improve the accuracy and relevance of generated text. [...] Traditional Retrieval-Augmented Generation (RAG) techniques often rely on vector similarity for searching information. GraphRAG enhances this by using graph-based methods, which can better connect disparate pieces of information and provide synthesized insights12.\\n\\n### Application to Private Data:\\n\\n[](https://github.com/0xZee/Graph-RAG#application-to-private-data) [...] Imagine you have a large collection of business reports and you want to extract key insights. Using GraphRAG, you can create a knowledge graph from these reports and then query the system to get detailed, contextually relevant answers about trends, key themes, and other insights.\\n\\nFor more detailed information, you can check out the GraphRAG repository and the Microsoft Research Blog Post.\\n\\nAbout\\n-----\\n\\nMicrosoft Graph-RAG Indexing and Quering\\n\\n### Topics', 'https://www.chitika.com/graph-based-retrieval-augmented-generation/': 'At the heart of RAG lies the retrieval pipeline, where the interplay between data indexing and query matching determines success. Graph-based indexing, for example, excels in capturing multi-hop relationships, enabling nuanced retrieval for tasks like drug discovery. Unlike flat databases, graph structures allow the system to traverse interconnected nodes, uncovering insights that linear methods often miss. [...] Scalability under complex data loads is crucial. Traditional RAG faces bottlenecks as datasets grow, while Graph RAG’s structured indexing enables efficient traversal in massive graphs. A leading investment firm, for example, uses Graph RAG to process billions of relationship traversals daily, maintaining sub-150ms latency. [...] Traditional RAG frameworks primarily rely on unstructured text data and vector-based retrieval methods, which can limit their ability to understand complex relationships between entities.\\n\\nIn contrast, graph-based RAG frameworks utilize structured knowledge graphs, enabling a more nuanced representation of relationships and context. This allows for enhanced semantic search, multi-hop reasoning, and the ability to handle complex or ambiguous queries more effectively.', 'https://www.deepset.ai/blog/graph-rag': 'Microsoft recently open-sourced [GraphRAG](https://microsoft.github.io/graphrag/), and it is proving to be a game changer in enhancing RAG techniques.\\n\\n\\u200d\\n\\n> **_By combining graph-based techniques at indexing and query time, GraphRAG is able to return much more informative and contextually relevant answers than RAG alone._**\\n\\n\\u200d', 'https://www.elastic.co/search-labs/blog/rag-graph-traversal': 'Graph-based RAG can provide several advantages over a traditional document-based approach: Unlike classic RAG, which only retrieves information based on individual documents, Graph RAG can highlight relationships between entities even if they do not co-occur in the same document. This is particularly useful for uncovering implicit connections. By relying on structured triplets (entity, relation, entity), a KG provides a synthesized and noise-free version of the database. This increases the [...] **Figure 9:** Querying the aggregated relations index: a quick way to know which nodes a node is connected to\\n\\nUsing a triple ES index structure, we implemented a hybrid document and graph-based RAG system using a single database engine. This approach enables efficient graph construction and retrieval without requiring additional infrastructure. [...] To overcome this limitation, researchers have proposed leveraging **Knowledge Graphs** to enhance RAG performance. Unlike document-based approaches, Knowledge Graphs allow for structured relationships between entities, enabling deeper and more contextual retrieval. However, seamlessly integrating Knowledge Graphs into RAG remains a challenge, particularly when using tools like Elasticsearch, which, while highly effective for document-based RAG, has not been conceived for graph-based', 'https://dataforest.ai/blog/vector-db-for-rag-information-retrieval-with-semantic-search': '### **Setting Up Your Indexing Strategy**\\n\\nIndexing in a RAG vector DB is the difference between a librarian having to check every single book to find what you need versus knowing exactly which shelf to look on. Without proper indexing, searching through millions of vectors would be painfully slow. By implementing benchmarking practices during indexing, you evaluate the effectiveness of retrieval times and optimize for better query performance. [...] RAG and vector DB come with specialized indexing techniques like HNSW and IVF to speed up searches. Instead of scanning each vector, these indexes apply weighting techniques to optimize the priority of vectors based on relevance metrics. For searching, it\\'s about finding the \"[nearest neighbors](https://dataforest.ai/glossary/k-nearest-neighbors-knn)\" or the most relevant vectors based on distance metrics. As for scalability, RAG vector DB is built to handle billions of entries while keeping [...] Using a tech provider, such as [DATAFOREST](https://dataforest.ai), makes working with RAG and vector DB way easier by taking care of the infrastructure, like scaling and hosting, so you donâ\\x80\\x99t have to worry about it. Providers come with built-in indexing and optimizations, giving you fast searches without needing to tweak everything yourself. They also offer handy monitoring tools to spot performance issues early. You can use their pre-trained models to create embeddings, which saves a ton of'}}}\n",
      "--  {'ask_question': {'messages': [AIMessage(content='This is incredibly insightful.  My next question is:  can you elaborate on the specific prompt engineering techniques that are particularly effective in maximizing the utilization of the expanded context window in these million-plus token models, and what are the key considerations for designing effective prompts in this context?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, name='Dr. Anya Sharma', id='run--651816dd-2901-420f-8657-e99db4c5bcfd-0', usage_metadata={'input_tokens': 1278, 'output_tokens': 57, 'total_tokens': 1335, 'input_token_details': {'cache_read': 0}})]}}\n",
      "--  {'answer_question': {'messages': [AIMessage(content=\"Effective prompt engineering for million-plus token context window LLMs requires a multi-faceted approach.  First, structuring prompts to explicitly guide the model's attention is crucial.  Techniques like specifying the desired output format, providing clear instructions, and using keywords or topic sentences to highlight relevant information within the vast context are highly effective.<sup>1, 2</sup>  Second, iterative prompting, where the initial prompt is refined based on the model's response, can significantly improve accuracy and efficiency.<sup>3</sup>  This allows for a more focused and targeted approach, leveraging the model's feedback to progressively narrow the search space.<sup>4</sup>  Third, techniques like chain-of-thought prompting, where the model is guided to break down complex tasks into smaller, more manageable steps, can be particularly beneficial for long-context tasks.<sup>5</sup>  This helps to maintain coherence and prevent the model from losing track of the overall goal.<sup>6</sup>  Fourth, incorporating explicit instructions to prioritize specific sections of the context or to focus on particular relationships between pieces of information can further enhance performance.<sup>7</sup>  Finally, careful consideration of prompt length is essential.  While larger context windows are available, excessively long prompts can lead to decreased performance due to the model's limitations in processing vast amounts of information.<sup>8</sup>  Therefore, a balance must be struck between providing sufficient context and avoiding overwhelming the model.<sup>9</sup>\\n\\nCitations:\\n\\n[1]: https://arxiv.org/abs/2502.12462\\n[2]: https://arxiv.org/html/2502.12462v1\\n[3]: https://community.openai.com/t/prompt-engineering-for-rag/621495\\n[4]: https://dev.to/shittu_olumide_/prompt-engineering-patterns-for-successful-rag-implementations-2m2e\\n[5]: https://docs.aws.amazon.com/prescriptive-guidance/latest/llm-prompt-engineering-best-practices/introduction.html\\n[6]: https://latitude-blog.ghost.io/blog/context-aware-prompt-scaling-key-concepts/\\n[7]: https://medium.com/@luvverma2011/optimizing-llms-best-practices-prompt-engineering-rag-and-fine-tuning-8def58af8dcc\\n[8]: https://mychen76.medium.com/revolutionizing-code-agents-with-multi-million-token-model-new-agentic-workflow-prompts-memory-141b3fc2c284\\n[9]: https://www.kolena.com/guides/llm-context-windows-why-they-matter-and-5-solutions-for-context-limits/\", additional_kwargs={}, response_metadata={}, name='Subject Matter Expert')], 'references': {'https://www.kolena.com/guides/llm-context-windows-why-they-matter-and-5-solutions-for-context-limits/': 'Prompt engineering involves designing prompts that effectively utilize the context window to produce the best possible responses from LLMs. This involves strategically structuring the input to include essential details while omitting irrelevant information. Techniques include: [...] This technique is particularly useful for tasks like document summarization or long-form text generation, where maintaining coherence and context is a priority. The sliding window method allows models to handle longer texts by piecing together these overlapping segments to create a cohesive output.\\n\\n### 2\\\\. Prompt Engineering [...] *   **Incremental context building:** Continuously appending relevant information from previous interactions to the prompt.\\n*   **Adaptive prompts:** Modifying the prompt based on the model’s previous responses to steer the conversation or task in the desired direction.\\n*   **Context trimming:** Removing less relevant parts of the prompt as new, more relevant information becomes available.\\n\\n### 4\\\\. Chunking and Summarizing', 'https://latitude-blog.ghost.io/blog/context-aware-prompt-scaling-key-concepts/': '### How can I effectively manage context windows when creating prompts for large language models?\\n\\nManaging context windows is crucial when crafting prompts for large language models (LLMs). To get the best results, start by **streamlining token usage** - include only the most relevant details and cut out anything unnecessary. Condensing or prioritizing key points ensures the model stays focused on what truly matters without getting sidetracked. [...] By managing context effectively, organizations can cut costs and improve performance in real-world applications. For instance, with GPT-4o charging $5.00 per million input tokens and $15.00 per million output tokens, mastering context window management can lead to substantial savings without sacrificing quality. Techniques like retrieval augmentation show that a 4,000-token window can rival the output of a 16,000-token model.\\n\\n### Key Takeaways [...] Designing prompts that scale effectively is a cornerstone of successful workflows with Large Language Models (LLMs). It bridges the gap between technical fine-tuning and practical implementation. To achieve this, adopting structured frameworks and identifying repeatable patterns are crucial. These approaches not only create consistency but also encourage collaboration and innovation.\\n\\n### Standardized Frameworks for Prompt Engineering', 'https://mychen76.medium.com/revolutionizing-code-agents-with-multi-million-token-model-new-agentic-workflow-prompts-memory-141b3fc2c284': 'figure: long context window for code agent\\n\\nObjective\\n=========\\n\\nIn this article, I want to explore the benefits of a million-token long context window, new prompting techniques from OpenAI to support agentic workflows, and a surprising discovery about XML tags in prompt engineering and Cline Memory Bank concepts to manage context window growth.\\n\\nContent\\n=======\\n\\n*   Why context size matters\\n\\nCreate an account to read\\xa0the\\xa0full\\xa0story.\\n\\n\\n---------------------------------------------', 'https://arxiv.org/html/2502.12462v1': 'In this research, we propose a novel method that emulates RAG through prompt engineering and chain-of-thought reasoning, aiming to enhance LLM capabilities in long-context comprehension while mitigating the drawbacks of standard retrieval. Our approach treats the model as both the retriever and the reasoner: first identifying relevant segments within a large context, then injecting explicit reasoning traces to simulate multi-hop retrieval. By doing so, we attempt to unify the best of both [...] This paper addresses the challenge of comprehending very long contexts in Large Language Models (LLMs) by proposing a method that emulates Retrieval Augmented Generation (RAG) through specialized prompt engineering and chain-of-thought (CoT) reasoning. While recent LLMs support over 100,000 tokens in a single prompt, simply enlarging context windows has not guaranteed robust multi-hop reasoning when key details are scattered across massive input. Our approach treats the model as both the [...] We presented a novel approach that emulates Retrieval-Augmented Generation (RAG) via specialized prompt engineering and chain-of-thought reasoning, all within a single forward pass of a large language model. By tagging relevant segments and guiding the model to perform multi-hop reasoning over them, we address key limitations of naive one-shot retrieval or purely large-context methods. Empirical evaluations on BABILong tasks demonstrate that our prompt-based solution can outperform both', 'https://dev.to/shittu_olumide_/prompt-engineering-patterns-for-successful-rag-implementations-2m2e': 'The guide covers various prompt patterns, such as Direct Retrieval, Chain of Thought, Context Enrichment, Instruction-Tuning, Persona-Based', 'https://community.openai.com/t/prompt-engineering-for-rag/621495': '![](https://sea2.discourse-cdn.com/openai1/user_avatar/community.openai.com/wip/48/300790_2.png)\\n\\nThe closest I’ve seen is negative instructions like, “If the answer isn’t contained here, say you don’t know.”\\n\\nIn general you want to avoid negative instructions in prompts. The model CAN follow negative instructions but it naturally favors positive instructions. For RAG I typically use a prompt format something along the lines of this:\\n\\nDOCUMENT:  \\n(document text)\\n\\nQUESTION:  \\n(users question) [...] INSTRUCTIONS:  \\nAnswer the users QUESTION using the DOCUMENT text above.  \\nKeep your answer ground in the facts of the DOCUMENT.  \\nIf the DOCUMENT doesn’t contain the facts to answer the QUESTION return {NONE}\\n\\nThe important thing to understand is that these models are fundamentally pattern recognizers. The more you show them a repeating pattern the more likely they are to follow your instructions.\\n\\nThe other tip is the less you say to the model the better. [...] ![](https://sea2.discourse-cdn.com/openai1/user_avatar/community.openai.com/wclayf/48/184514_2.png)\\n\\nI wonder if people have experimented with putting instructions at beginning, and then repeating instructions at the end, with something like, “Remember, from above, your instructions are as follows: ${INSTRUCTIONS}”. I have a hunch that might improve results.\\n\\nThe danger with repeated chunks is that it can confuse the model.', 'https://arxiv.org/abs/2502.12462': '# Title:Emulating Retrieval Augmented Generation via Prompt Engineering for Enhanced Long Context Comprehension in LLMs', 'https://medium.com/@luvverma2011/optimizing-llms-best-practices-prompt-engineering-rag-and-fine-tuning-8def58af8dcc': '7. Thus, simple fine-tuning + RAG combined with simple prompt engineering brought the model accuracy to 83.5%\\n\\n***If you like, please read and clap!!***\\n\\n[***LinkedIn***](https://www.linkedin.com/in/luvverma/)\\n\\n--\\n\\n--\\n\\n![Luv Verma](https://miro.medium.com/v2/resize:fill:96:96/1*C-5V5gdfQB3ZR6Xvq_4HOw.jpeg)\\n![Luv Verma](https://miro.medium.com/v2/resize:fill:128:128/1*C-5V5gdfQB3ZR6Xvq_4HOw.jpeg)\\n\\n## Written by Luv Verma\\n\\n## No responses yet\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy [...] 4. **Devise proper evaluation strategies**\\n\\n## Best Practices in Fine-Tuning + RAG\\n\\n2. Next, use RAG to inject relevant knowledge into the context needed to be maximized, but do not over-saturate the context\\n\\n# OpenAI tested best practices established above on Yale Semantic Parsing and Text-to-SQL Challenge\\n\\n**Use case description: Given a natural language question and a database schema, can the model produce a semantically correct SQL query?**\\n\\n![]() [...] 7. It can be a possibility, that retrieval might not be as good as one wants, then go back to RAG and optimize the RAG again.\\n\\n# **Optimization via Prompt Engineering**\\n\\n2. **After going through the above steps, they can be extended to**\\n\\n3. **Prompt engineering is good for**\\n\\n**4. Not good for**\\n\\n# **Optimization via Retrieval Augmented Generation (RAG)**\\n\\n![]()\\n\\nRAG overview is shown above. It gives the model access to the domain-specific context.\\n\\n2. **Not good for**\\n\\n# RAG use case', 'https://docs.aws.amazon.com/prescriptive-guidance/latest/llm-prompt-engineering-best-practices/introduction.html': 'This guide outlines security guardrails for mitigating prompt engineering and prompt\\ninjection attacks. These guardrails are compatible with various model providers and prompt\\ntemplates, but require additional customization for specific models.\\n\\n## Targeted business outcomes\\n\\nSignificantly improve the prompt-level security of LLM-powered\\nretrieval-augmented generation (RAG) applications against a variety of common\\nattack patterns while maintaining high accuracy for non-malicious\\nqueries.'}}}\n",
      "--  {'ask_question': {'messages': [AIMessage(content='Thank you so much for your help!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, name='Dr. Anya Sharma', id='run--01b6aa93-e934-4565-9ec5-85c031c0ceb0-0', usage_metadata={'input_tokens': 1940, 'output_tokens': 9, 'total_tokens': 1949, 'input_token_details': {'cache_read': 0}})]}}\n",
      "--  {'answer_question': {'messages': [AIMessage(content='Current limitations of million-token context window models include the computational cost of processing such large inputs, the potential for reduced accuracy due to the difficulty of maintaining coherence over extensive contexts, and challenges in effectively utilizing all the provided information.  These models may struggle to fully leverage all available data, even within the expanded window, leading to performance degradation.  Furthermore, the increased memory requirements necessitate specialized hardware and efficient indexing strategies to avoid performance bottlenecks.  Finally, data quality remains a critical factor; even with larger windows, poor data can lead to inaccurate or misleading outputs.\\n\\nCitations:\\n\\n[1]: https://newmr.org/blog/what-is-a-context-window-and-why-does-it-matter/\\n[2]: https://sandar-ali.medium.com/analysis-of-llama-4s-10-million-token-context-window-claim-9e68ee5abcde\\n[3]: https://www.ibm.com/think/topics/context-window\\n[4]: https://www.perplexity.ai/page/context-window-limitations-of-FKpx7M_ITz2rKXLFG1kNiQ', additional_kwargs={}, response_metadata={}, name='Subject Matter Expert')], 'references': {'https://sandar-ali.medium.com/analysis-of-llama-4s-10-million-token-context-window-claim-9e68ee5abcde': '# Current Hardware Limitations\\n\\nThe above simple calculation raises a valid point about the limitations of current hardware to handle the memory demands associated with a 10 million token context window. Modern LLMs require significant computational resources and memory capacity, particularly GPU memory, for efficient processing. [...] I was astonished when the claim of a 10 million token context window for LLaMA 4 was announced. However, I wanted to check about the technical plausibility of such a large window given the architecture’s attention mechanism, the memory requirements for the KV-cache, and the limitations of current hardware. I analyzed the code provided at the Github [repository](https://github.com/meta-llama/llama-models/blob/main/models/llama4/model.py) . As per my understanding, it seems that LLaMA 4 employs a [...] hybrid attention mechanism, featuring both a global attention without positional encoding and a local attention computed in chunks. This observation forms the basis of my concerns about the model’s ability to handle an exceptionally long context. This article will delve into a technical examination of these concerns, analyzing the memory complexity of different attention mechanisms, the role of positional encoding, the scaling of the KV-cache with context window size, and the limitations', 'https://www.ibm.com/think/topics/context-window': 'The average context window of a large language model has grown exponentially since the original generative pretrained transformers (GPTs) were released. To date, each successive generation of LLMs has typically entailed significantly longer context lengths. At present, the largest context window offered by a prominent commercial model is over 1 million tokens. It remains to be seen whether context windows will continue to expand or if we’re already approaching the upper limit of practical [...] [Google’s Gemini](https://www.ibm.com/think/topics/google-gemini) series of models offers what is currently the largest context window amongst commercial language models. Gemini 1.5 Pro, Google’s flagship model, offers a context length of up to 2 million tokens. Other Gemini 1.5 models, such as Gemini 1.5 Flash, have a context window of 1 million tokens.\\n\\n#### **Anthropic’s Claude models** [...] For reference, here are the current context lengths offered by some commonly cited models and model families as of October 2024.\\n\\n#### **OpenAI’s GPT series:**\\n\\nThe new o1 model family likewise offers a context window of 128,000 tokens, though they offer greater output context length.\\n\\n#### **Meta Llama models**', 'https://newmr.org/blog/what-is-a-context-window-and-why-does-it-matter/': '**A rule-of-thumb example**I find that an 80-minute focus group typically produces about 9,000 tokens of transcript. A 100,000-token window can therefore hold roughly ten such groups at once. With a two-million-token window, you are unlikely to hit the limit in normal qualitative projects.\\n\\n**Why it matters**\\n\\nIn short, context-window size sets the ceiling on how much *context* the AI can actively process, so it is worth checking before you choose a model for your next project.\\n\\n### *Related* [...] window of 128,000 tokens, GPT-3o has a window of 200,000 tokens, and the current flagship has a one-million-token context window. [...] **Rapid growth in size**When ChatGPT was released to the public in November 2022, it offered a 4,096-token context window. Today, windows of 100,000 tokens or more are common. Claude 3, for example, has 200,000 tokens, while the largest of Google’s Gemini models has up to two million tokens. (LLMs count in *tokens* rather than words; 1 word is approximately 1.3 tokens.) Different models from the same company have different context window sizes. For example, OpenAI ChatGPT GPT-4o has a context', 'https://www.perplexity.ai/page/context-window-limitations-of-FKpx7M_ITz2rKXLFG1kNiQ': \"For instance, GPT-3's context window of 2,048 tokens (approximately 1,500 words) can be quickly exhausted in complex tasks or extended dialogues\"}}}\n"
     ]
    }
   ],
   "source": [
    "final_step = None\n",
    "initial_state = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": [AIMessage(\n",
    "        content=f\"So you said you were writing an article on {example_topic}?\",\n",
    "        name=\"Subject Matter Expert\"\n",
    "    )]\n",
    "}\n",
    "\n",
    "async for step in interview_graph.astream(initial_state):\n",
    "    name=next(iter(step))\n",
    "    # print(step)\n",
    "    print(\"-- \", step)\n",
    "    final_step = step\n",
    "    \n",
    "\n",
    "final_state = next(iter(final_step.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c92ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='Current limitations of million-token context window models include the computational cost of processing such large inputs, the potential for reduced accuracy due to the difficulty of maintaining coherence over extensive contexts, and challenges in effectively utilizing all the provided information.  These models may struggle to fully leverage all available data, even within the expanded window, leading to performance degradation.  Furthermore, the increased memory requirements necessitate specialized hardware and efficient indexing strategies to avoid performance bottlenecks.  Finally, data quality remains a critical factor; even with larger windows, poor data can lead to inaccurate or misleading outputs.\\n\\nCitations:\\n\\n[1]: https://newmr.org/blog/what-is-a-context-window-and-why-does-it-matter/\\n[2]: https://sandar-ali.medium.com/analysis-of-llama-4s-10-million-token-context-window-claim-9e68ee5abcde\\n[3]: https://www.ibm.com/think/topics/context-window\\n[4]: https://www.perplexity.ai/page/context-window-limitations-of-FKpx7M_ITz2rKXLFG1kNiQ', additional_kwargs={}, response_metadata={}, name='Subject Matter Expert')],\n",
       " 'references': {'https://sandar-ali.medium.com/analysis-of-llama-4s-10-million-token-context-window-claim-9e68ee5abcde': '# Current Hardware Limitations\\n\\nThe above simple calculation raises a valid point about the limitations of current hardware to handle the memory demands associated with a 10 million token context window. Modern LLMs require significant computational resources and memory capacity, particularly GPU memory, for efficient processing. [...] I was astonished when the claim of a 10 million token context window for LLaMA 4 was announced. However, I wanted to check about the technical plausibility of such a large window given the architecture’s attention mechanism, the memory requirements for the KV-cache, and the limitations of current hardware. I analyzed the code provided at the Github [repository](https://github.com/meta-llama/llama-models/blob/main/models/llama4/model.py) . As per my understanding, it seems that LLaMA 4 employs a [...] hybrid attention mechanism, featuring both a global attention without positional encoding and a local attention computed in chunks. This observation forms the basis of my concerns about the model’s ability to handle an exceptionally long context. This article will delve into a technical examination of these concerns, analyzing the memory complexity of different attention mechanisms, the role of positional encoding, the scaling of the KV-cache with context window size, and the limitations',\n",
       "  'https://www.ibm.com/think/topics/context-window': 'The average context window of a large language model has grown exponentially since the original generative pretrained transformers (GPTs) were released. To date, each successive generation of LLMs has typically entailed significantly longer context lengths. At present, the largest context window offered by a prominent commercial model is over 1 million tokens. It remains to be seen whether context windows will continue to expand or if we’re already approaching the upper limit of practical [...] [Google’s Gemini](https://www.ibm.com/think/topics/google-gemini) series of models offers what is currently the largest context window amongst commercial language models. Gemini 1.5 Pro, Google’s flagship model, offers a context length of up to 2 million tokens. Other Gemini 1.5 models, such as Gemini 1.5 Flash, have a context window of 1 million tokens.\\n\\n#### **Anthropic’s Claude models** [...] For reference, here are the current context lengths offered by some commonly cited models and model families as of October 2024.\\n\\n#### **OpenAI’s GPT series:**\\n\\nThe new o1 model family likewise offers a context window of 128,000 tokens, though they offer greater output context length.\\n\\n#### **Meta Llama models**',\n",
       "  'https://newmr.org/blog/what-is-a-context-window-and-why-does-it-matter/': '**A rule-of-thumb example**I find that an 80-minute focus group typically produces about 9,000 tokens of transcript. A 100,000-token window can therefore hold roughly ten such groups at once. With a two-million-token window, you are unlikely to hit the limit in normal qualitative projects.\\n\\n**Why it matters**\\n\\nIn short, context-window size sets the ceiling on how much *context* the AI can actively process, so it is worth checking before you choose a model for your next project.\\n\\n### *Related* [...] window of 128,000 tokens, GPT-3o has a window of 200,000 tokens, and the current flagship has a one-million-token context window. [...] **Rapid growth in size**When ChatGPT was released to the public in November 2022, it offered a 4,096-token context window. Today, windows of 100,000 tokens or more are common. Claude 3, for example, has 200,000 tokens, while the largest of Google’s Gemini models has up to two million tokens. (LLMs count in *tokens* rather than words; 1 word is approximately 1.3 tokens.) Different models from the same company have different context window sizes. For example, OpenAI ChatGPT GPT-4o has a context',\n",
       "  'https://www.perplexity.ai/page/context-window-limitations-of-FKpx7M_ITz2rKXLFG1kNiQ': \"For instance, GPT-3's context window of 2,048 tokens (approximately 1,500 words) can be quickly exhausted in complex tasks or extended dialogues\"}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3784105",
   "metadata": {},
   "source": [
    "# Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f9763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a42073e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': '5d65824d-1af9-433a-b0a9-e4c0dde2dfd9', 'source': 'https://sandar-ali.medium.com/analysis-of-llama-4s-10-million-token-context-window-claim-9e68ee5abcde'}, page_content='# Current Hardware Limitations\\n\\nThe above simple calculation raises a valid point about the limitations of current hardware to handle the memory demands associated with a 10 million token context window. Modern LLMs require significant computational resources and memory capacity, particularly GPU memory, for efficient processing. [...] I was astonished when the claim of a 10 million token context window for LLaMA 4 was announced. However, I wanted to check about the technical plausibility of such a large window given the architecture’s attention mechanism, the memory requirements for the KV-cache, and the limitations of current hardware. I analyzed the code provided at the Github [repository](https://github.com/meta-llama/llama-models/blob/main/models/llama4/model.py) . As per my understanding, it seems that LLaMA 4 employs a [...] hybrid attention mechanism, featuring both a global attention without positional encoding and a local attention computed in chunks. This observation forms the basis of my concerns about the model’s ability to handle an exceptionally long context. This article will delve into a technical examination of these concerns, analyzing the memory complexity of different attention mechanisms, the role of positional encoding, the scaling of the KV-cache with context window size, and the limitations'),\n",
       " Document(metadata={'id': '1ca3e685-2883-4807-86b8-5e6f7c00c3bc', 'source': 'https://www.ibm.com/think/topics/context-window'}, page_content='The average context window of a large language model has grown exponentially since the original generative pretrained transformers (GPTs) were released. To date, each successive generation of LLMs has typically entailed significantly longer context lengths. At present, the largest context window offered by a prominent commercial model is over 1 million tokens. It remains to be seen whether context windows will continue to expand or if we’re already approaching the upper limit of practical [...] [Google’s Gemini](https://www.ibm.com/think/topics/google-gemini) series of models offers what is currently the largest context window amongst commercial language models. Gemini 1.5 Pro, Google’s flagship model, offers a context length of up to 2 million tokens. Other Gemini 1.5 models, such as Gemini 1.5 Flash, have a context window of 1 million tokens.\\n\\n#### **Anthropic’s Claude models** [...] For reference, here are the current context lengths offered by some commonly cited models and model families as of October 2024.\\n\\n#### **OpenAI’s GPT series:**\\n\\nThe new o1 model family likewise offers a context window of 128,000 tokens, though they offer greater output context length.\\n\\n#### **Meta Llama models**'),\n",
       " Document(metadata={'id': '63d11bb3-298e-45f6-a3bc-8f791b16fc72', 'source': 'https://newmr.org/blog/what-is-a-context-window-and-why-does-it-matter/'}, page_content='**A rule-of-thumb example**I find that an 80-minute focus group typically produces about 9,000 tokens of transcript. A 100,000-token window can therefore hold roughly ten such groups at once. With a two-million-token window, you are unlikely to hit the limit in normal qualitative projects.\\n\\n**Why it matters**\\n\\nIn short, context-window size sets the ceiling on how much *context* the AI can actively process, so it is worth checking before you choose a model for your next project.\\n\\n### *Related* [...] window of 128,000 tokens, GPT-3o has a window of 200,000 tokens, and the current flagship has a one-million-token context window. [...] **Rapid growth in size**When ChatGPT was released to the public in November 2022, it offered a 4,096-token context window. Today, windows of 100,000 tokens or more are common. Claude 3, for example, has 200,000 tokens, while the largest of Google’s Gemini models has up to two million tokens. (LLMs count in *tokens* rather than words; 1 word is approximately 1.3 tokens.) Different models from the same company have different context window sizes. For example, OpenAI ChatGPT GPT-4o has a context'),\n",
       " Document(metadata={'id': '61e270ca-6418-4df7-9423-4e93948dc341', 'source': 'https://www.perplexity.ai/page/context-window-limitations-of-FKpx7M_ITz2rKXLFG1kNiQ'}, page_content=\"For instance, GPT-3's context window of 2,048 tokens (approximately 1,500 words) can be quickly exhausted in complex tasks or extended dialogues\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "reference_docs = [\n",
    "    Document(page_content=v, metadata={\"source\": k})\n",
    "    for k, v in final_state['references'].items()\n",
    "]\n",
    "\n",
    "\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    \n",
    "    reference_docs,\n",
    "    embedding=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"What's a long context LLM anyway?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f6e5a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of Million-Plus Token Context Window Language Models on RAG\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Introduction to the topic of million-plus token context window language models and their potential effects on Retrieval Augmented Generation (RAG).\n",
      "\n",
      "### Retrieval Augmented Generation (RAG)\n",
      "\n",
      "Brief overview of RAG and its current limitations.\n",
      "\n",
      "### Million-Plus Token Context Window Language Models\n",
      "\n",
      "Explanation of million-plus token context window language models and their capabilities.\n",
      "\n",
      "### Thesis Statement\n",
      "\n",
      "Thesis statement:  How these larger context windows impact the effectiveness and efficiency of RAG systems.\n",
      "\n",
      "## Advantages\n",
      "\n",
      "Discussion of the advantages of using million-plus token context windows in RAG.\n",
      "\n",
      "### Enhanced Contextual Understanding\n",
      "\n",
      "Improved context understanding and reduced need for retrieval.\n",
      "\n",
      "### Reduced Hallucinations and Improved Accuracy\n",
      "\n",
      "Fewer hallucinations and more accurate responses due to access to more relevant information.\n",
      "\n",
      "### Handling Complex Queries\n",
      "\n",
      "Potential for handling more complex queries and tasks.\n",
      "\n",
      "### Real-World Applications\n",
      "\n",
      "Examples of specific applications where this advantage is significant.\n",
      "\n",
      "## Challenges and Limitations\n",
      "\n",
      "Examination of the challenges and limitations associated with using such large context windows.\n",
      "\n",
      "### Computational Cost\n",
      "\n",
      "Computational cost and resource requirements.\n",
      "\n",
      "### Inference Speed\n",
      "\n",
      "Potential for slower inference times.\n",
      "\n",
      "### Memory Management\n",
      "\n",
      "Memory management issues.\n",
      "\n",
      "### Training and Fine-tuning\n",
      "\n",
      "Difficulty in training and fine-tuning these models.\n",
      "\n",
      "### Bias and Other Negative Effects\n",
      "\n",
      "Potential for increased bias or other negative effects from larger datasets.\n",
      "\n",
      "## Comparative Analysis\n",
      "\n",
      "Comparison of RAG systems using traditional context windows versus million-plus token context windows.\n",
      "\n",
      "### Evaluation Metrics\n",
      "\n",
      "Metrics for evaluating performance (e.g., accuracy, efficiency, hallucination rate).\n",
      "\n",
      "### Existing Research and Benchmarks\n",
      "\n",
      "Presentation of existing research and benchmarks.\n",
      "\n",
      "### Trade-offs\n",
      "\n",
      "Analysis of the trade-offs between context window size and performance.\n",
      "\n",
      "## Future Directions\n",
      "\n",
      "Future directions and potential research areas related to million-plus token context window language models and RAG.\n",
      "\n",
      "### Architectural Innovations\n",
      "\n",
      "Exploration of new architectures and techniques to address the challenges.\n",
      "\n",
      "### Efficient Training and Inference\n",
      "\n",
      "Development of more efficient training and inference methods.\n",
      "\n",
      "### Impact on Different RAG Systems\n",
      "\n",
      "Investigation of the impact on different types of RAG systems.\n",
      "\n",
      "### Ethical Considerations\n",
      "\n",
      "Discussion of ethical considerations.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Summary of the key findings and conclusions.\n",
      "\n",
      "### Recap of Advantages and Disadvantages\n",
      "\n",
      "Recap of the advantages and disadvantages of using million-plus token context windows in RAG.\n",
      "\n",
      "### Overall Assessment\n",
      "\n",
      "Overall assessment of the impact on the field of RAG.\n",
      "\n",
      "### Suggestions for Future Research\n",
      "\n",
      "Suggestions for future research and development.\n"
     ]
    }
   ],
   "source": [
    "direct_gen_outline_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a Wikipedia writer. Write an outline for a Wikipedia article about user-provided topic. Be comprehensive and specific.\"\n",
    "    ),\n",
    "    ('user', \"{topic}\")\n",
    "])\n",
    "\n",
    "class Subsection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
    "    description: str = Field(..., title=\"Content of the subsection\")\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.description}\".strip()\n",
    "    \n",
    "class Section(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    description: str = Field(..., title=\"Content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(default=None, title=\"Title and description for each subsection of the Wikipedia page.\")\n",
    "    \n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(f\"### {subsection.subsection_title}\\n\\n{subsection.description}\"\n",
    "                                  \n",
    "                                  for subsection in self.subsections or [])\n",
    "        return f\"## {self.section_title}\\n\\n{self.description}\\n\\n{subsections}\".strip()\n",
    "    \n",
    "class Outline(BaseModel):\n",
    "    page_title: str = Field(..., title=\"Title of the Wikipedia page\")\n",
    "    sections: List[Section] = Field(\n",
    "        default_factory=list,\n",
    "        title=\"Title and description for each section of the Wikipedia page.\"\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        sections=  \"\\n\\n\".join(section.as_str for section in self.sections)\n",
    "        return f\"# {self.page_title}\\n\\n{sections}\".strip()\n",
    "    \n",
    "\n",
    "generate_outline_direct= direct_gen_outline_prompt | fast_llm.with_structured_output(Outline)\n",
    "\n",
    "initila_outline = generate_outline_direct.invoke({\n",
    "    'topic': example_topic\n",
    "})\n",
    "\n",
    "print(initila_outline.as_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0eafe2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of Million-Plus Token Context Window Language Models on RAG\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Introduction to the topic of million-plus token context window language models and their potential effects on Retrieval Augmented Generation (RAG).\n",
      "\n",
      "## Retrieval Augmented Generation (RAG)\n",
      "\n",
      "Brief overview of RAG and its current limitations.\n",
      "\n",
      "## Million-Plus Token Context Window Language Models\n",
      "\n",
      "Explanation of million-plus token context window language models and their capabilities.\n",
      "\n",
      "## Thesis Statement\n",
      "\n",
      "Thesis statement:  How these larger context windows impact the effectiveness and efficiency of RAG systems.\n",
      "\n",
      "## Advantages\n",
      "\n",
      "Discussion of the advantages of using million-plus token context windows in RAG.\n",
      "\n",
      "### Enhanced Contextual Understanding\n",
      "\n",
      "Improved context understanding and reduced need for retrieval.\n",
      "\n",
      "### Reduced Hallucinations and Improved Accuracy\n",
      "\n",
      "Fewer hallucinations and more accurate responses due to access to more relevant information.\n",
      "\n",
      "### Handling Complex Queries\n",
      "\n",
      "Potential for handling more complex queries and tasks.\n",
      "\n",
      "### Real-World Applications\n",
      "\n",
      "Examples of specific applications where this advantage is significant.\n",
      "\n",
      "## Challenges and Limitations\n",
      "\n",
      "Examination of the challenges and limitations associated with using such large context windows.\n",
      "\n",
      "### Computational Cost\n",
      "\n",
      "Computational cost and resource requirements.\n",
      "\n",
      "### Inference Speed\n",
      "\n",
      "Potential for slower inference times.\n",
      "\n",
      "### Memory Management\n",
      "\n",
      "Memory management issues.\n",
      "\n",
      "### Training and Fine-tuning\n",
      "\n",
      "Difficulty in training and fine-tuning these models.\n",
      "\n",
      "### Bias and Other Negative Effects\n",
      "\n",
      "Potential for increased bias or other negative effects from larger datasets.\n",
      "\n",
      "### Effective Utilization of Information\n",
      "\n",
      "Models may struggle to fully leverage all available data, even within the expanded window, leading to performance degradation.\n",
      "\n",
      "### Impact of Data Quality\n",
      "\n",
      "Data quality remains a critical factor; even with larger windows, poor data can lead to inaccurate or misleading outputs.\n",
      "\n",
      "## Comparative Analysis\n",
      "\n",
      "Comparison of RAG systems using traditional context windows versus million-plus token context windows.\n",
      "\n",
      "### Evaluation Metrics\n",
      "\n",
      "Metrics for evaluating performance (e.g., accuracy, efficiency, hallucination rate).\n",
      "\n",
      "### Existing Research and Benchmarks\n",
      "\n",
      "Presentation of existing research and benchmarks.\n",
      "\n",
      "### Trade-offs\n",
      "\n",
      "Analysis of the trade-offs between context window size and performance.\n",
      "\n",
      "## Future Directions\n",
      "\n",
      "Future directions and potential research areas related to million-plus token context window language models and RAG.\n",
      "\n",
      "### Architectural Innovations\n",
      "\n",
      "Exploration of new architectures and techniques to address the challenges.\n",
      "\n",
      "### Efficient Training and Inference\n",
      "\n",
      "Development of more efficient training and inference methods.\n",
      "\n",
      "### Impact on Different RAG Systems\n",
      "\n",
      "Investigation of the impact on different types of RAG systems.\n",
      "\n",
      "### Ethical Considerations\n",
      "\n",
      "Discussion of ethical considerations.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Summary of the key findings and conclusions.\n",
      "\n",
      "### Recap of Advantages and Disadvantages\n",
      "\n",
      "Recap of the advantages and disadvantages of using million-plus token context windows in RAG.\n",
      "\n",
      "### Overall Assessment\n",
      "\n",
      "Overall assessment of the impact on the field of RAG.\n",
      "\n",
      "### Suggestions for Future Research\n",
      "\n",
      "Suggestions for future research and development.\n"
     ]
    }
   ],
   "source": [
    "refine_outline_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        'system',\n",
    "        \"\"\"You are a Wikipedia writer. You have gathered information from an expert and search engines. Now, you are refining yhe outline of the Wikipedia page. \\\n",
    "            You need to make sure that the outline is comprehensive and specific.\n",
    "            \\\n",
    "            Topic you are writing about: {topic}\n",
    "            \n",
    "            Old outline:\n",
    "            {old_outline}\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        'user',\n",
    "        \"Refine the outline. based on your conversations with subject matter experts:\\n\\nConversations:\\n{conversations}\\n\\nWrite the refined Wikipedia outline.\"\n",
    "    )\n",
    "])\n",
    "\n",
    "refine_outline_chain = refine_outline_prompt | long_context_llm.with_structured_output(Outline)\n",
    "\n",
    "refined_outline = refine_outline_chain.invoke({\n",
    "    'topic':example_topic,\n",
    "    \"old_outline\": initila_outline.as_str,\n",
    "    \"conversations\": \"\\n\\n\".join(f\"### {m.name}\\n\\n{m.content}\" for m in final_state['messages'])\n",
    "})\n",
    "print(refined_outline.as_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3317fe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Retrieval Augmented Generation (RAG)\n",
      "\n",
      "Retrieval Augmented Generation (RAG) is a framework for improving the quality of responses generated by large language models (LLMs) by providing them with access to external knowledge sources. Instead of relying solely on the information contained within their parameters, RAG models retrieve relevant information from a database or other knowledge repository and incorporate it into the generation process. This allows the LLM to provide more accurate, up-to-date, and contextually relevant answers, especially for topics or queries that require information beyond the model's original training data.\n",
      "\n",
      "Current RAG systems face limitations primarily due to the context window constraints of the underlying LLMs. A context window refers to the amount of text that a language model can process at one time. When the information needed to answer a query exceeds this window, the RAG system must select a subset of the available knowledge to provide to the LLM. This selection process can be imperfect, leading to the retrieval of irrelevant information or the exclusion of crucial context, ultimately impacting the quality of the generated response. For instance, GPT-3's context window of 2,048 tokens (approximately 1,500 words) can be quickly exhausted in complex tasks or extended dialogues.[0] https://www.perplexity.ai/page/context-window-limitations-of-FKpx7M_ITz2rKXLFG1kNiQ\n"
     ]
    }
   ],
   "source": [
    "class SubSection(BaseModel):\n",
    "   subsection_title: str = Field(..., title='Title of the subsection') \n",
    "   content: str = Field(..., title='Full content of the subsection. Include [#] citations to the cited sources where relevant.')\n",
    "   @property\n",
    "   def as_str(self) -> str:\n",
    "      return f\"### {self.subsection_title}\\n\\n{self.content}\".strip()\n",
    "   \n",
    "class WikiSection(BaseModel):\n",
    "   section_title: str = Field(..., title='Title of the section')\n",
    "   content: str = Field(..., title='Full content of the section. ')\n",
    "   subsections: Optional[List[SubSection]] = Field(default=None, title='Title and content for each subsection of the Wikipedia page.')\n",
    "   citations: List[str] =Field(default_factory=list)\n",
    "   @property\n",
    "   def as_str(self) -> str:\n",
    "      subsections = \"\\n\\n\".join(subsection.as_str\n",
    "                                  for subsection in self.subsections or [])\n",
    "      citations = \"\\n\".join(f\" [{i}] {cit}\" for i, cit in enumerate(self.citations))\n",
    "      return f\"## {self.section_title}\\n\\n{self.content}\\n\\n{subsections}\".strip() + f\"\\n\\n{citations}\".strip()\n",
    "   \n",
    "section_writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert Wikipedia writer. Complete your assigned WikiSection from the following outline:\\n\\n\"\n",
    "            \"{outline}\\n\\nCite your sources, using the following references:\\n\\n<Documents>\\n{docs}\\n<Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"Write the full WikiSection for the {section} section.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "async def retrieve(inputs: dict):\n",
    "   docs = await retriever.ainvoke(inputs['topic']+\": \"+inputs['section'])\n",
    "   formatted = \"\\n\".join(\n",
    "      [\n",
    "         f'<Document href=\"{doc.metadata['source']}\"/>\\n{doc.page_content}</Document>'\n",
    "         for doc in docs\n",
    "      ]\n",
    "   )\n",
    "   return {\"docs\":formatted, **inputs}\n",
    "\n",
    "\n",
    "section_writer = (\n",
    "   retrieve\n",
    "   | section_writer_prompt\n",
    "   | long_context_llm.with_structured_output(WikiSection)\n",
    ")\n",
    "\n",
    "section = await section_writer.ainvoke({\n",
    "   \"outline\": refined_outline.as_str,\n",
    "   \"topic\": example_topic,\n",
    "   \"section\": refined_outline.sections[1].section_title\n",
    "})\n",
    "\n",
    "print(section.as_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0560e462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Retrieval Augmented Generation (RAG)\n",
      "\n",
      "Retrieval Augmented Generation (RAG) is a framework for improving the quality of responses generated by large language models (LLMs) by providing them with access to external knowledge sources. Instead of relying solely on the information contained within their parameters, RAG models retrieve relevant information from a database or other knowledge repository and incorporate it into the generation process. This allows the LLM to provide more accurate, up-to-date, and contextually relevant answers, especially for topics or queries that require information beyond the model's original training data.\n",
      "\n",
      "Current RAG systems face limitations primarily due to the context window constraints of the underlying LLMs. A context window refers to the amount of text that a language model can process at one time. When the information needed to answer a query exceeds this window, the RAG system must select a subset of the available knowledge to provide to the LLM. This selection process can be imperfect, leading to the retrieval of irrelevant information or the exclusion of crucial context, ultimately impacting the quality of the generated response. For instance, GPT-3's context window of 2,048 tokens (approximately 1,500 words) can be quickly exhausted in complex tasks or extended dialogues.[0] https://www.perplexity.ai/page/context-window-limitations-of-FKpx7M_ITz2rKXLFG1kNiQ\n",
      "```markdown\n",
      "{{short description|Explores the impact of large context window language models on Retrieval Augmented Generation (RAG) systems.}}\n",
      "{{Infobox technology\n",
      "| name = Impact of Million-Plus Token Context Window Language Models on RAG\n",
      "| image =\n",
      "| caption =\n",
      "| invented =\n",
      "| inventor =\n",
      "| date =\n",
      "| status = Emerging\n",
      "| predecessor =\n",
      "| successor =\n",
      "}}\n",
      "\n",
      "**Impact of Million-Plus Token Context Window Language Models on RAG** refers to the transformative effect that language models (LLMs) with extremely large context windows (exceeding one million tokens) have on Retrieval Augmented Generation (RAG) systems. These expanded context windows allow RAG systems to process and utilize significantly more information during the retrieval and generation phases, leading to improved accuracy, coherence, and contextual understanding in the generated outputs.\n",
      "\n",
      "== Retrieval Augmented Generation (RAG) ==\n",
      "\n",
      "Retrieval Augmented Generation (RAG) is a framework for improving the quality of responses generated by large language models (LLMs) by providing them with access to external knowledge sources. Instead of relying solely on the information contained within their parameters, RAG models retrieve relevant information from a database or other knowledge repository and incorporate it into the generation process. This allows the LLM to provide more accurate, up-to-date, and contextually relevant answers, especially for topics or queries that require information beyond the model's original training data.\n",
      "\n",
      "Current RAG systems face limitations primarily due to the context window constraints of the underlying LLMs. A context window refers to the amount of text that a language model can process at one time. When the information needed to answer a query exceeds this window, the RAG system must select a subset of the available knowledge to provide to the LLM. This selection process can be imperfect, leading to the retrieval of irrelevant information or the exclusion of crucial context, ultimately impacting the quality of the generated response. For instance, GPT-3's context window of 2,048 tokens (approximately 1,500 words) can be quickly exhausted in complex tasks or extended dialogues.[1]\n",
      "\n",
      "== Impact of Million-Plus Token Context Windows ==\n",
      "\n",
      "The advent of LLMs with million-plus token context windows addresses many of the limitations inherent in traditional RAG systems. The increased context capacity allows for:\n",
      "\n",
      "*   **Improved Information Retrieval:** RAG systems can now retrieve and process a much larger volume of potentially relevant documents or data snippets. This reduces the risk of missing crucial information due to context window limitations.\n",
      "*   **Enhanced Contextual Understanding:** The LLM can maintain a more comprehensive understanding of the overall context, including long-range dependencies and relationships between different pieces of information. This leads to more coherent and contextually appropriate responses.\n",
      "*   **Reduced Need for Information Filtering:** With a larger context window, the RAG system can afford to be less aggressive in filtering retrieved information. This reduces the risk of discarding valuable context that might be relevant to the query.\n",
      "*   **Better Handling of Complex Queries:** Complex queries that require synthesizing information from multiple sources or understanding intricate relationships can be handled more effectively with a larger context window. The LLM has access to more of the necessary information to formulate a comprehensive and accurate response.\n",
      "*   **Improved Long-Form Generation:** For tasks such as generating long articles, reports, or stories, a large context window allows the LLM to maintain coherence and consistency over extended passages.\n",
      "\n",
      "== Challenges and Considerations ==\n",
      "\n",
      "While million-plus token context windows offer significant advantages, they also present new challenges:\n",
      "\n",
      "*   **Computational Cost:** Processing extremely large context windows can be computationally expensive, requiring significant memory and processing power. This can impact the scalability and cost-effectiveness of RAG systems.\n",
      "*   **\"Lost in the Middle\" Effect:** Research suggests that LLMs may struggle to effectively utilize information located in the middle of very long context windows, exhibiting a tendency to focus on information at the beginning and end.[2] This phenomenon, known as the \"lost in the middle\" effect, needs to be addressed through architectural improvements or training strategies.\n",
      "*   **Noise and Irrelevant Information:** A larger context window can also increase the risk of including irrelevant or noisy information, which can degrade the quality of the generated response. Effective retrieval and filtering mechanisms are still crucial.\n",
      "*   **Evaluation Metrics:** Existing evaluation metrics for RAG systems may not be adequate for assessing the performance of systems with extremely large context windows. New metrics are needed to accurately measure the ability of these systems to utilize and synthesize information from vast amounts of text.\n",
      "\n",
      "== Future Directions ==\n",
      "\n",
      "The development of million-plus token context window LLMs is an ongoing area of research. Future directions include:\n",
      "\n",
      "*   **Architectural Improvements:** Developing new LLM architectures that can more effectively process and utilize information from very long context windows, mitigating the \"lost in the middle\" effect.\n",
      "*   **Efficient Retrieval Techniques:** Designing more efficient retrieval algorithms that can quickly identify the most relevant information from large knowledge repositories.\n",
      "*   **Contextual Compression:** Exploring techniques for compressing or summarizing information within the context window without losing crucial details.\n",
      "*   **Adaptive Context Windows:** Developing systems that can dynamically adjust the size of the context window based on the complexity of the query and the available resources.\n",
      "*   **Integration with Knowledge Graphs:** Combining large context windows with knowledge graphs to provide LLMs with structured knowledge and reasoning capabilities.\n",
      "\n",
      "== References ==\n",
      "{{reflist}}\n",
      "\n",
      "== External Links ==\n",
      "*   [https://www.perplexity.ai/page/context-window-limitations-of-FKpx7M_ITz2rKXLFG1kNiQ Context Window Limitations of Large Language Models]\n",
      "```\n",
      "\n",
      "**References**\n",
      "\n",
      "[1] \"Context Window Limitations of Large Language Models.\" Perplexity AI. [https://www.perplexity.ai/page/context-window-limitations-of-FKpx7M_ITz2rKXLFG1kNiQ](https://www.perplexity.ai/page/context-window-limitations-of-FKpx7M_ITz2rKXLFG1kNiQ)\n",
      "[2] Liu, Nelson F., et al. \"Lost in the Middle: How Language Models Use Information Throughout the Context Window.\" *arXiv preprint arXiv:2307.03172* (2023).\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert Wikipedia author. Write the complete wiki article on {topic} using the following section drafts:\\n\\n\"\n",
    "            \"{draft}\\n\\nStrictly follow Wikipedia format guidelines.\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            'Write the complete Wiki article using markdown format. Organize citations using footnotes like \"[1]\",'\n",
    "            \" avoiding duplicates in the footer. Include URLs in the footer.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "writer = writer_prompt | long_context_llm | StrOutputParser()\n",
    "print(section.as_str)\n",
    "for tok in writer.stream({\"topic\": example_topic, \"draft\":section.as_str}):\n",
    "    print(tok, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08e3e4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    outline: Outline\n",
    "    editors: List[Editor]\n",
    "    interview_results: List[InterviewState]\n",
    "    sections: List[WikiSection]\n",
    "    article: str\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def initialize_research(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    coros = (\n",
    "        generate_outline_direct.ainvoke({\"topic\": topic}),\n",
    "        survey_subjects.ainvoke(topic),\n",
    "    )\n",
    "    results = await asyncio.gather(*coros)\n",
    "    return {\n",
    "        **state,\n",
    "        \"outline\": results[0],\n",
    "        \"editors\": results[1].editors,\n",
    "    }\n",
    "\n",
    "\n",
    "async def conduct_interviews(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    initial_states = [\n",
    "        {\n",
    "            \"editor\": editor,\n",
    "            \"messages\": [\n",
    "                AIMessage(\n",
    "                    content=f\"So you said you were writing an article on {topic}?\",\n",
    "                    name=\"Subject_Matter_Expert\",\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        for editor in state[\"editors\"]\n",
    "    ]\n",
    "    # We call in to the sub-graph here to parallelize the interviews\n",
    "    interview_results = await interview_graph.abatch(initial_states)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"interview_results\": interview_results,\n",
    "    }\n",
    "\n",
    "def format_conversation(interview_state):\n",
    "    messages = interview_state[\"messages\"]\n",
    "    convo = \"\\n\".join(f\"{message.name}: {message.content}\" for message in messages)\n",
    "    return f\"Conversation with {interview_state['editor'].name}\\n\\n\"+convo\n",
    "\n",
    "\n",
    "async def refine_outline(state:ResearchState):\n",
    "    convos= \"\\n\\n\".join(\n",
    "        [\n",
    "            format_conversation(interview_state)\n",
    "            for interview_state in state['interview_results']\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    updated_outline = await refine_outline_chain.ainvoke({\n",
    "        'topic': state['topic'],\n",
    "        'old_outline': state['outline'].as_str,\n",
    "        \"conversations\": convos\n",
    "    })\n",
    "    return {**state, \"outline\": updated_outline}\n",
    "\n",
    "async def index_references(state:ResearchState):\n",
    "    all_docs = []\n",
    "    for interview_state in state['interview_results']:\n",
    "        reference_docs = [\n",
    "            Document(page_content=v, metadata={\"source\": k})\n",
    "            for k, v in interview_state['references'].items()\n",
    "        ]\n",
    "        all_docs.extend(reference_docs)\n",
    "    await vectorstore.aadd_documents(all_docs)\n",
    "    return state\n",
    "\n",
    "async def write_sections(state:ResearchState):\n",
    "    outline = state['outline']\n",
    "    sections = await section_writer.abatch([\n",
    "        {\n",
    "            \"outline\": outline.as_str,\n",
    "            \"section\": section.section_title,\n",
    "            \"topic\": state['topic']\n",
    "        }\n",
    "        for section in outline.sections\n",
    "    ])\n",
    "    return {\n",
    "        **state,\n",
    "        \"sections\": sections,\n",
    "    }\n",
    "async def write_article(state:ResearchState):\n",
    "    topic = state['topic']\n",
    "    sections = state['sections']\n",
    "    draft = \"\\n\\n\".join(section.as_str for section in sections)\n",
    "    article = await writer.ainvoke({\"topic\": topic, \"draft\": draft})\n",
    "    return {\n",
    "        **state,\n",
    "        \"article\": article\n",
    "    }\n",
    "\n",
    "\n",
    "builder_of_storm = StateGraph(ResearchState)\n",
    "\n",
    "nodes = [\n",
    "    (\"init_research\", initialize_research),\n",
    "    (\"conduct_interviews\", conduct_interviews),\n",
    "    (\"refine_outline\", refine_outline),\n",
    "    (\"index_references\", index_references),\n",
    "    (\"write_sections\", write_sections),\n",
    "    (\"write_article\", write_article),\n",
    "]\n",
    "for i in range(len(nodes)):\n",
    "    name, node = nodes[i]\n",
    "    builder_of_storm.add_node(name, node)\n",
    "    if i > 0:\n",
    "        builder_of_storm.add_edge(nodes[i - 1][0], name)\n",
    "\n",
    "builder_of_storm.set_entry_point(nodes[0][0])\n",
    "builder_of_storm.set_finish_point(nodes[-1][0])\n",
    "storm = builder_of_storm.compile()\n",
    "from IPython.display import Image\n",
    "Image(storm.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ef0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_research\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', sections=[Section(section_title='Introduction', description='Introduction to Large Language Model (LLM) inference and its challenges.', subs\n",
      "conduct_interviews\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', sections=[Section(section_title='Introduction', description='Introduction to Large Language Model (LLM) inference and its challenges.', subs\n",
      "refine_outline\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', sections=[Section(section_title='Introduction', description='Introduction to Large Language Model (LLM) inference and its challenges.', subs\n",
      "index_references\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', sections=[Section(section_title='Introduction', description='Introduction to Large Language Model (LLM) inference and its challenges.', subs\n",
      "write_sections\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', sections=[Section(section_title='Introduction', description='Introduction to Large Language Model (LLM) inference and its challenges.', subs\n",
      "write_article\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', sections=[Section(section_title='Introduction', description='Introduction to Large Language Model (LLM) inference and its challenges.', subs\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "## LLM Inference Platforms: Groq, NVIDIA, Llama.cpp, and the Future\n",
       "\n",
       "### Introduction\n",
       "\n",
       "Large Language Models (LLMs) have emerged as a transformative technology, demonstrating remarkable capabilities in natural language understanding and generation. However, deploying these models for real-time applications presents significant computational challenges. This section provides an overview of LLMs, defines LLM inference, and discusses the challenges associated with it.\n",
       "\n",
       "#### What are LLMs?\n",
       "\n",
       "LLMs are deep learning models with a large number of parameters, trained on vast amounts of text data. They are capable of performing a wide range of tasks, including text generation, translation, question answering, and code generation. Examples include GPT-3, LLaMA, and BERT.\n",
       "\n",
       "#### LLM Inference: Definition and Significance\n",
       "\n",
       "LLM inference refers to the process of using a trained LLM to generate outputs for new inputs. It is the deployment phase where the model is used to make predictions or generate text in real-world applications. Efficient LLM inference is crucial for enabling interactive applications and services powered by these models.\n",
       "\n",
       "#### Challenges in LLM Inference\n",
       "\n",
       "LLM inference faces several challenges, including high latency, limited throughput, and substantial memory requirements. The computational intensity of LLMs demands specialized hardware and software optimization techniques to achieve acceptable performance. Memory bandwidth and power consumption also pose significant constraints.\n",
       "\n",
       "### Groq and LLM Inference\n",
       "\n",
       "Groq offers a unique approach to LLM inference with its Tensor Streaming Processor (TSP) architecture. Unlike traditional GPUs, Groq's architecture emphasizes deterministic data flow and high memory bandwidth, which are particularly well-suited for the demands of LLM inference.\n",
       "\n",
       "#### Groq's Hardware Architecture\n",
       "\n",
       "Groq's hardware architecture is centered around the Tensor Streaming Processor (TSP), now referred to as the Language Processing Unit (LPU). The LPU is designed for high computational throughput and low latency, crucial for LLM inference. A key feature is its deterministic data flow, which allows for predictable performance and efficient resource utilization. The architecture prioritizes high memory bandwidth to keep the processing units fed with data, minimizing bottlenecks. However, each chip has limited on-chip memory (200MB), requiring racks to run larger LLMs.\n",
       "\n",
       "#### Groq's Software Stack and Optimization Techniques\n",
       "\n",
       "Groq's software stack includes a compiler that plays a critical role in scheduling and resource allocation. The compiler optimizes the execution of LLMs on the TSP architecture, taking advantage of its deterministic nature. The deterministic programming model simplifies optimization and allows for predictable performance. Groq's architecture doesn't get faster for batch sizes >1.\n",
       "\n",
       "#### Performance Benchmarks and Comparisons\n",
       "\n",
       "Groq has demonstrated impressive performance benchmarks, particularly in terms of latency. A single Groq LPU card can outperform expensive cloud GPU instances on LLM serving. Groq's architecture shines in scenarios where low latency is paramount.\n",
       "\n",
       "#### Power Efficiency and Thermal Characteristics\n",
       "\n",
       "Groq's architecture is designed for power efficiency. By minimizing data movement and maximizing resource utilization, the TSP-based systems can achieve competitive performance per watt. The reduced need for over-provisioning for batch size contributes to power savings.\n",
       "\n",
       "#### Handling Variability and Stochasticity\n",
       "\n",
       "Groq addresses the variability and stochasticity of LLM inference workloads through its scheduling and resource allocation mechanisms. The deterministic nature of the TSP architecture allows for precise control over execution, mitigating the impact of variability.\n",
       "\n",
       "### NVIDIA and LLM Inference\n",
       "\n",
       "NVIDIA's approach to LLM inference leverages its powerful GPUs and a comprehensive software ecosystem. Their strategy centers around maximizing parallelism and providing developers with robust tools for optimization.\n",
       "\n",
       "#### NVIDIA GPUs and LLM Inference\n",
       "\n",
       "NVIDIA GPUs, particularly architectures like H100 and A100, are designed for massively parallel computation, making them well-suited for the demands of LLM inference. These GPUs contain thousands of cores that can simultaneously perform the matrix multiplications and other operations that are fundamental to deep learning. The high memory bandwidth and large memory capacity of NVIDIA GPUs also allow them to handle large models and datasets efficiently.\n",
       "\n",
       "#### NVIDIA's Software Ecosystem and Optimization\n",
       "\n",
       "NVIDIA's software ecosystem, including CUDA and TensorRT, provides developers with the tools to optimize LLMs for inference. CUDA is a parallel computing platform and programming model that allows developers to harness the power of NVIDIA GPUs for general-purpose computing. TensorRT is an SDK for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning applications.\n",
       "\n",
       "The advantages of using CUDA and TensorRT include: broad hardware compatibility, mature software tools, and extensive community support. Challenges include: the complexity of CUDA programming and the need for specialized knowledge to optimize models for TensorRT.\n",
       "\n",
       "#### Performance Benchmarks and Comparisons\n",
       "\n",
       "NVIDIA GPUs offer high performance and scalability for LLM inference. Benchmarks consistently show that NVIDIA GPUs can deliver high throughput and low latency for a wide range of LLMs. However, power consumption can be a concern, especially for large models. The performance and power efficiency of NVIDIA GPUs vary depending on the specific model, the size of the LLM, and the optimization techniques used. According to a report on NextPlatform, when comparing against Groq's LPU, NVIDIA's GPUs can do an inference in one-tenth the time at one tenth the cost of generating the tokens. It takes the Nvidia GPU somewhere on the order of 10 joules to 30 joules to generate tokens in a response. [1]\n",
       "\n",
       "#### Software Development and Tooling\n",
       "\n",
       "Optimizing LLMs for inference on NVIDIA GPUs involves several steps, including: model quantization, graph optimization, and kernel fusion. NVIDIA provides a suite of tools and libraries to assist developers with these tasks. Key challenges include: balancing performance and accuracy, managing memory usage, and adapting to new model architectures. Advantages include: access to a wide range of pre-trained models and optimized kernels, and the ability to leverage NVIDIA's expertise in deep learning.\n",
       "\n",
       "#### Scalability\n",
       "\n",
       "NVIDIA's GPU architecture scales effectively using technologies like NVLink and NVSwitch for multi-GPU systems. NVLink provides high-bandwidth, low-latency interconnects between GPUs, allowing them to share data and coordinate computations efficiently. NVSwitch enables all-to-all communication between GPUs in a system. NVIDIA also offers mature software tools for distributed training and inference across clusters of GPUs, making it possible to scale LLM inference to handle large workloads.\n",
       "\n",
       "### Llamma.cpp and Open-Source LLM Inference\n",
       "\n",
       "Llamma.cpp has emerged as a significant player in the open-source LLM inference landscape, providing a platform for running large language models on a variety of hardware, including CPUs. It democratizes access to LLMs by enabling local inference, reducing reliance on proprietary hardware and cloud services.\n",
       "\n",
       "#### Introduction to Llamma.cpp\n",
       "\n",
       "Llamma.cpp is a project focused on enabling efficient inference of large language models (LLMs) using C++. Its primary goal is to make LLMs accessible to a wider audience by allowing them to run on consumer-grade hardware, including laptops and desktops, without requiring specialized GPUs. This is achieved through a combination of optimization techniques and support for various quantization methods.\n",
       "\n",
       "#### Architecture and Implementation\n",
       "\n",
       "Llamma.cpp is written in C++ for performance and portability. It leverages techniques like quantization (reducing the precision of weights and activations) to minimize memory footprint and computational requirements. The architecture is designed to be modular, allowing for easy integration of new models and hardware platforms. It supports various platforms, including x86, ARM, and Apple Silicon. The core implementation focuses on efficient matrix multiplication and other linear algebra operations crucial for LLM inference.\n",
       "\n",
       "#### Performance and Limitations\n",
       "\n",
       "Llamma.cpp's performance varies depending on the hardware and model used. While it may not match the throughput of high-end GPUs, it offers a viable option for local inference, especially for smaller models or when running on resource-constrained devices. Limitations include slower inference speeds compared to GPU-accelerated solutions and potential accuracy degradation due to quantization. However, ongoing development continuously improves its performance and expands its capabilities.\n",
       "\n",
       "#### Comparison with Other Open-Source Solutions\n",
       "\n",
       "Several other open-source LLM inference solutions exist, such as those based on ONNX Runtime or optimized for specific hardware architectures. Llamma.cpp distinguishes itself through its focus on simplicity, portability, and ease of use. It often serves as a reference implementation and a starting point for researchers and developers exploring LLM inference optimization techniques. Compared to some more complex solutions, Llamma.cpp is easier to set up and use, making it attractive to users with limited technical expertise.\n",
       "\n",
       "#### Quantization Techniques\n",
       "\n",
       "Llamma.cpp employs various quantization techniques to reduce the memory footprint and computational demands of LLMs. Quantization involves converting the floating-point weights and activations of a model to lower-precision integer representations (e.g., 8-bit or 4-bit integers). This reduces the model size and speeds up inference, but it can also lead to a loss of accuracy. The effectiveness and impact on accuracy of these quantization techniques can vary depending on the hardware platform. Groq's TSP, with its deterministic data flow, might exhibit different quantization behavior compared to NVIDIA GPUs, which rely on massively parallel architectures. Further research and experimentation are needed to fully understand these differences.\n",
       "\n",
       "### Comparative Analysis\n",
       "\n",
       "A comprehensive comparison of Groq, NVIDIA, and Llamma.cpp reveals distinct advantages and disadvantages across various parameters, including performance, cost, ease of use, and accessibility. Each platform caters to different needs and use cases, making a direct comparison complex but insightful.\n",
       "\n",
       "#### Performance, Cost, Ease of Use, and Accessibility\n",
       "\n",
       "When considering performance, Groq stands out for its low latency inference, particularly beneficial for real-time applications. NVIDIA GPUs, on the other hand, offer high throughput and scalability, making them suitable for large-scale deployments. Llamma.cpp provides a more accessible entry point for local inference, but its performance is generally lower compared to the other two, especially with larger models. In terms of cost, Llamma.cpp offers the most cost-effective solution as it leverages existing hardware. Groq's LPU cards can be expensive, but they offer a compelling performance per dollar for specific workloads. NVIDIA GPUs vary in price, with high-end models being a significant investment. Ease of use is another differentiating factor. NVIDIA boasts a mature software ecosystem with CUDA and TensorRT, providing extensive tools and libraries for developers. Llamma.cpp is relatively easy to set up and use, especially for those familiar with C++. Groq's software stack, while powerful, may require a steeper learning curve. Accessibility is also a key consideration. NVIDIA GPUs are widely available and supported by major cloud providers. Llamma.cpp can be run on a wide range of hardware, making it highly accessible. Groq's hardware is less readily available, potentially limiting its accessibility.\n",
       "\n",
       "#### Strengths and Weaknesses in Different Use Cases\n",
       "\n",
       "Groq excels in use cases demanding ultra-low latency, such as real-time language translation or interactive AI applications. Its deterministic data flow and high memory bandwidth make it well-suited for these scenarios. NVIDIA GPUs shine in applications requiring high throughput and scalability, such as large-scale language model serving or training. Their massively parallel architecture and mature software ecosystem provide the necessary tools for these tasks. Llamma.cpp is ideal for local inference and experimentation, allowing users to run LLMs on their own hardware without relying on cloud services. However, it may not be suitable for production environments with high performance requirements. A key weakness of Groq is the limited on-chip memory, necessitating racks of chips to run larger LLMs. NVIDIA's weakness includes underutilization if batch sizes are not optimized, leading to wasted power. Llamma.cpp's primary weakness is its performance ceiling compared to dedicated hardware solutions.\n",
       "\n",
       "#### Power Efficiency Comparison\n",
       "\n",
       "Groq's LPU is designed for power efficiency, particularly when running at its optimal batch size, avoiding the over-provisioning issues that can plague GPU deployments. NVIDIA GPUs, while powerful, can consume significant power, especially when not fully utilized. Llamma.cpp's power consumption depends on the underlying hardware, but it generally consumes less power than dedicated GPU or LPU solutions. The key architectural factors contributing to Groq's power efficiency include its deterministic data flow and elimination of external memory access bottlenecks. By minimizing data movement and maximizing on-chip computation, Groq reduces energy consumption. NVIDIA GPUs, on the other hand, rely on a more traditional architecture with external memory access, which can be a significant source of power consumption.\n",
       "\n",
       "#### Software Stacks and Development Tools\n",
       "\n",
       "NVIDIA's CUDA and TensorRT provide a rich set of tools and libraries for optimizing LLMs, along with extensive community support and a wide range of pre-trained models. Groq's software stack, while offering a deterministic programming model, may have a smaller community and fewer pre-trained models readily available. This can increase the initial effort required to deploy and optimize LLMs on Groq's platform. Llamma.cpp benefits from its simplicity and ease of use, but it may lack the advanced optimization features and extensive tooling available in NVIDIA's ecosystem. The choice of software stack and development tools depends on the specific requirements of the project, the available expertise, and the desired level of control over the optimization process.\n",
       "\n",
       "### Future of LLM Inference\n",
       "\n",
       "The field of LLM inference is rapidly evolving, driven by the increasing demand for faster, more efficient, and more accessible AI. Several emerging technologies and trends are poised to significantly impact the future of LLM inference, shaping the landscape for both hardware and software solutions.\n",
       "\n",
       "#### Emerging Technologies and Their Impact\n",
       "\n",
       "Several emerging technologies promise to revolutionize LLM inference.  **Specialized Hardware:**  Continued development of specialized hardware like Groq's LPU and other ASICs (Application-Specific Integrated Circuits) will likely lead to further performance gains and power efficiency improvements.  **Quantization and Pruning:** Advances in model compression techniques like quantization (reducing the precision of weights) and pruning (removing less important connections) will enable smaller, faster models that require less memory and compute.  **Novel Architectures:** Exploration of novel neural network architectures, such as Mixture of Experts (MoE), could lead to more efficient and scalable models.  **Near-Memory Computing:** Architectures that bring computation closer to memory can reduce data movement bottlenecks, a major factor in LLM inference latency.\n",
       "\n",
       "#### Future Landscape and Predictions\n",
       "\n",
       "The future of LLM inference is likely to be characterized by a few key trends.  **Increased Specialization:**  We can expect to see further specialization of hardware and software for LLM inference, with solutions tailored to specific model sizes, architectures, and deployment scenarios.  **Edge Inference:**  The ability to run LLMs on edge devices (e.g., smartphones, IoT devices) will become increasingly important, enabling real-time AI applications with low latency and enhanced privacy.  **Democratization of AI:**  Open-source solutions like Llama.cpp will continue to play a crucial role in democratizing access to LLMs, allowing individuals and smaller organizations to experiment with and deploy these models without relying on expensive proprietary platforms.  **Cloud vs. On-Premise:**  The balance between cloud-based and on-premise LLM inference will depend on factors such as cost, latency requirements, data security concerns, and regulatory constraints.  It's likely that both deployment models will coexist, with cloud solutions being favored for large-scale, general-purpose applications and on-premise solutions being preferred for latency-sensitive or data-private use cases.\n",
       "\n",
       "#### Ethical and Environmental Implications\n",
       "\n",
       "The increasing use of LLMs raises important ethical and environmental considerations.  **Energy Consumption:**  LLM inference can be energy-intensive, particularly when using large models and running them at scale. Groq's architecture, with its focus on efficiency, offers a potential advantage in reducing the carbon footprint of LLM deployments compared to traditional GPU-based solutions. Open-source solutions like Llama.cpp can also contribute to sustainability by enabling inference on less powerful hardware.  **Bias and Fairness:**  LLMs can perpetuate and amplify biases present in their training data, leading to unfair or discriminatory outcomes. It is crucial to carefully evaluate and mitigate these biases to ensure that LLMs are used responsibly.  **Accessibility:**  The cost and complexity of LLM inference can create barriers to access, potentially exacerbating existing inequalities. Efforts to democratize access to LLMs through open-source solutions and more efficient hardware are essential to ensure that the benefits of AI are shared broadly.\n",
       "\n",
       "#### Security Implications\n",
       "\n",
       "Security is a paramount concern in LLM inference.  **Malicious Code Injection:**  LLMs are vulnerable to prompt injection attacks, where malicious actors can manipulate the model's behavior by crafting carefully designed prompts. Robust input validation and sanitization techniques are needed to mitigate this risk.  **Data Breaches:**  LLMs can inadvertently leak sensitive information if they are not properly secured. Access control mechanisms, data encryption, and privacy-preserving techniques are essential to protect confidential data.  **Model Tampering:**  Adversaries could attempt to tamper with the LLM itself, either by modifying its weights or by injecting malicious code. Model integrity checks and secure deployment practices are needed to prevent such attacks. The choice of platform can also impact security. Open-source solutions like Llama.cpp offer greater transparency and control, but they also require more expertise to secure properly. Proprietary platforms like Groq and NVIDIA may offer more robust security features, but they also involve a greater degree of trust in the vendor.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "In conclusion, the landscape of LLM inference is rapidly evolving, with diverse platforms like Groq, NVIDIA, and Llamma.cpp offering distinct advantages and disadvantages. Groq's LPU-based architecture excels in low-latency inference due to its deterministic data flow and high memory bandwidth, making it suitable for real-time applications. NVIDIA's GPUs, with their massively parallel architecture and mature software ecosystem, provide high throughput and scalability, catering to large-scale deployments. Llamma.cpp, as an open-source solution, offers flexibility and accessibility, empowering researchers and developers to experiment with LLM inference on commodity hardware.\n",
       "\n",
       "The choice of platform depends heavily on the specific use case, budget, and performance requirements. Groq shines in latency-sensitive applications, while NVIDIA dominates in throughput-demanding scenarios. Llamma.cpp provides a cost-effective and customizable solution for smaller-scale deployments and research purposes.\n",
       "\n",
       "Looking ahead, emerging technologies like specialized AI accelerators and advanced quantization techniques promise to further optimize LLM inference. The future landscape will likely be shaped by a combination of hardware and software innovations, with a focus on improving efficiency, reducing costs, and expanding accessibility. Ethical and environmental considerations will also play a crucial role in shaping the development and deployment of LLM inference platforms, encouraging the adoption of energy-efficient solutions and responsible AI practices. Security considerations around malicious code injection and data breaches will also continue to be paramount.\n",
       "\n",
       "Ultimately, the ongoing advancements in LLM inference will pave the way for more widespread adoption of AI-powered applications across various industries, transforming how we interact with technology and solve complex problems.\n",
       "\n",
       "### References\n",
       "\n",
       "[1]  \"Groq Says It Can Deploy 1 Million AI Inference Chips in Two Years.\" *The Next Platform*, 27 Nov. 2023, https://www.nextplatform.com/2023/11/27/groq-says-it-can-deploy-1-million-ai-inference-chips-in-two-years/.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = None\n",
    "async for step in storm.astream({\"topic\":\"Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\"}):\n",
    "    name=next(iter(step))\n",
    "    print(name)\n",
    "    print(\"-- \", str(step[name])[:300])\n",
    "    results = step[name]\n",
    "\n",
    "from IPython.display import Markdown\n",
    "article = results['article']\n",
    "Markdown(article.replace(\"\\n#\", \"\\n##\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f01777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1dbc5e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "## LLM Inference: Groq, NVIDIA, Llamma.cpp, and the Future\n",
       "\n",
       "### Introduction\n",
       "\n",
       "Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, capable of generating human-quality text, translating languages, and answering complex questions. However, efficiently deploying these models for real-time applications presents significant challenges. This section introduces the concept of LLM inference, its importance, and the hurdles that must be overcome to make LLMs accessible and practical.\n",
       "\n",
       "#### What are LLMs?\n",
       "\n",
       "LLMs are deep learning models with a large number of parameters, trained on vast amounts of text data. They leverage transformer architectures to understand and generate text, enabling applications such as chatbots, content creation, and code generation.\n",
       "\n",
       "#### LLM Inference: Definition and Significance\n",
       "\n",
       "LLM inference refers to the process of using a trained LLM to generate predictions or outputs for new input data. It is the crucial step that allows us to utilize the knowledge and capabilities learned by the model during training. Efficient inference is essential for deploying LLMs in real-world applications, as it directly impacts the user experience and the cost of serving the model.\n",
       "\n",
       "#### Challenges in LLM Inference\n",
       "\n",
       "LLM inference faces several challenges, including high computational costs, memory limitations, and latency requirements. The sheer size of LLMs, with billions or even trillions of parameters, demands significant computational resources. Minimizing latency, the time it takes to generate a response, is crucial for interactive applications. Furthermore, memory constraints can limit the size of models that can be deployed on a given hardware platform. Throughput, or the number of requests that can be processed per unit of time, is also a key consideration for serving a large number of users.\n",
       "\n",
       "#### Ethical Considerations and Societal Impact\n",
       "\n",
       "The increasing capabilities and deployment of AI accelerators and LLMs raise important ethical considerations. Accessibility is paramount; ensuring that the benefits of these technologies are available to a wide range of users and developers is crucial to avoid exacerbating existing inequalities. Bias in training data can lead to LLMs generating discriminatory or unfair outputs, highlighting the need for careful data curation and model evaluation. The environmental impact of training and deploying large models, particularly in terms of energy consumption, must also be addressed to promote sustainable AI development.\n",
       "\n",
       "### Groq and LLM Inference\n",
       "\n",
       "Groq is emerging as a significant player in the LLM inference landscape with its Tensor Streaming Processor (TSP) architecture. Unlike traditional GPUs, Groq's architecture is designed from the ground up for the sequential nature of language processing, potentially offering advantages in latency and throughput for LLM inference. Groq's solution focuses on providing high throughput, low latency, and lower cost for LLM inference compared to NVIDIA GPUs.\n",
       "\n",
       "Groq's API streamlines integration, allowing developers to use its LPU and open-source models in their applications without complex hardware setup or LLM expertise. The company offers several LLM models through its API, catering to diverse needs. Groq's LPU provides configurable hardware components, allowing developers to fine-tune the architecture for specific LLM models and tasks, optimizing performance and resource utilization.\n",
       "\n",
       "#### Groq's Hardware Architecture: Tensor Streaming Processor (TSP)\n",
       "\n",
       "Groq's hardware architecture centers around the Tensor Streaming Processor (TSP). The TSP is designed to minimize data movement and maximize computational density, which are critical for LLM inference. The architecture incorporates a large amount of SRAM memory close to the compute units, reducing the need to access external memory and improving performance. This design choice contrasts with GPUs, which often rely on large, fast main memory but can be bottlenecked by memory access latency. Groq's architecture is wide, relatively slow, and low power, achieving high performance through massive parallelism across many units.\n",
       "\n",
       "#### Groq's Software Stack and Optimization Techniques\n",
       "\n",
       "Groq offers a software stack that includes a compiler designed for deterministic execution. This compiler plays a crucial role in optimizing LLMs for the TSP architecture. The software stack aims to simplify the deployment of LLMs on Groq's hardware, abstracting away the complexities of the underlying hardware.\n",
       "\n",
       "#### Performance Benchmarks and Comparisons\n",
       "\n",
       "Groq has demonstrated impressive performance benchmarks, particularly in latency. Benchmarks show Groq's LPU Inference Engine achieving significantly faster inference speeds compared to NVIDIA GPUs for certain LLM workloads [2]. Groq's architecture shines in scenarios where low latency is paramount. For example, in LLAMA 2 benchmarks, Groq has shown the ability to process tokens at a significantly faster rate than NVIDIA GPUs [2]. However, it's important to consider the specific model, batch size, and inference scenario when comparing performance, as GPUs may excel in different situations.\n",
       "\n",
       "#### Limitations of Groq's LPU Technology\n",
       "\n",
       "Despite its strengths, Groq's LPU technology has limitations. The software ecosystem surrounding Groq's hardware is still maturing compared to NVIDIA's well-established CUDA ecosystem. Power efficiency is another consideration, as the 14nm chip may face challenges compared to newer GPU architectures in terms of power consumption per inference. Further, while Groq's architecture excels in certain LLM inference tasks, its applicability to other AI workloads may be more limited than that of general-purpose GPUs.\n",
       "\n",
       "#### Groq's TruePoint Quantization\n",
       "\n",
       "Groq utilizes a proprietary quantization technique called \"TruePoint.\" Quantization is a method of reducing the precision of numerical representations in a model (e.g., from 32-bit floating point to 8-bit integer), which can significantly improve performance and reduce memory footprint. TruePoint aims to minimize the accuracy loss associated with quantization, allowing Groq to achieve high performance without sacrificing model quality [0]. The specifics of the TruePoint technique are not widely publicized, but it is a key component of Groq's overall optimization strategy.\n",
       "\n",
       "### NVIDIA and LLM Inference\n",
       "\n",
       "#### NVIDIA GPUs and LLM Inference: Ampere/Hopper Architecture\n",
       "\n",
       "NVIDIA's hardware (GPUs) play a significant role in LLM inference, particularly the Ampere and Hopper architectures. These architectures incorporate Tensor Cores (Ampere) and Matrix Cores (Hopper), specialized hardware units designed to accelerate matrix multiplication operations, which are fundamental to deep learning and LLM inference. The Hopper architecture further enhances performance with features like the Transformer Engine, specifically optimized for transformer models.\n",
       "\n",
       "#### NVIDIA's Software Ecosystem and Optimization\n",
       "\n",
       "NVIDIA's software ecosystem, including CUDA and TensorRT, provides a comprehensive suite of tools for optimizing LLMs for inference. CUDA is a parallel computing platform and programming model that allows developers to leverage the power of NVIDIA GPUs. TensorRT is an SDK for high-performance deep learning inference, optimizing models for deployment in production environments. Optimization techniques include tensor parallelism (distributing tensors across multiple GPUs), pipeline parallelism (dividing the model into stages and processing them concurrently), and model quantization (reducing the precision of model weights to improve performance and reduce memory footprint).\n",
       "\n",
       "#### Performance Benchmarks and Comparisons\n",
       "\n",
       "NVIDIA GPUs consistently demonstrate strong performance in LLM inference benchmarks. The exact performance depends on the specific GPU model, LLM architecture, batch size, and optimization techniques used. NVIDIA provides detailed performance benchmarks for various LLMs and inference scenarios on its website and in technical publications. These benchmarks often compare NVIDIA GPUs against other platforms, highlighting their advantages in terms of latency and throughput.\n",
       "\n",
       "#### NVIDIA's Quantization Methods (PTQ, QAT)\n",
       "\n",
       "NVIDIA supports various quantization methods, including post-training quantization (PTQ) and quantization-aware training (QAT). PTQ involves quantizing a pre-trained model without further training, offering a simple and fast way to improve performance. QAT, on the other hand, involves training the model with quantization in mind, leading to better accuracy compared to PTQ but requiring more computational resources. Both methods offer trade-offs between accuracy and efficiency, allowing developers to choose the best approach for their specific needs.\n",
       "\n",
       "### Llamma.cpp and Open-Source LLM Inference\n",
       "\n",
       "Llamma.cpp has emerged as a significant player in the open-source LLM inference landscape. It distinguishes itself by enabling efficient inference on commodity hardware, making LLMs more accessible to a wider audience.\n",
       "\n",
       "#### Introduction to Llamma.cpp\n",
       "\n",
       "Llamma.cpp is a project focused on running large language models (LLMs) with minimal hardware requirements. Its primary goal is to enable efficient LLM inference on readily available consumer hardware, such as CPUs and lower-end GPUs, without relying on specialized AI accelerators. This makes it possible for individuals and smaller organizations to experiment with and deploy LLMs without significant capital investment.\n",
       "\n",
       "#### Architecture and Implementation\n",
       "\n",
       "Llamma.cpp achieves its efficiency through a combination of techniques. It's written in C++, allowing for low-level optimization and direct hardware control. Key implementation details include:\n",
       "\n",
       "*   **Quantization:** Llamma.cpp heavily utilizes quantization techniques, reducing the precision of model weights (e.g., 4-bit, 2-bit quantization). This significantly reduces memory footprint and computational requirements, enabling models to run on devices with limited resources.\n",
       "*   **Optimization Strategies:** The library employs various optimization strategies, such as kernel fusion and optimized matrix multiplication routines, to maximize performance on target hardware.\n",
       "*   **CPU and GPU Support:** Llamma.cpp supports both CPU and GPU execution, allowing users to choose the best option based on their hardware configuration. GPU support is often implemented via libraries like CUDA or Metal.\n",
       "\n",
       "#### Performance and Limitations\n",
       "\n",
       "The performance of Llamma.cpp depends heavily on the model size, quantization level, and hardware used. While it enables inference on resource-constrained devices, there are trade-offs:\n",
       "\n",
       "*   **Accuracy vs. Efficiency:** Lower precision quantization (e.g., 2-bit) leads to greater efficiency but can also reduce model accuracy. Users must carefully balance these factors.\n",
       "*   **Hardware Requirements:** While Llamma.cpp can run on CPUs, performance is generally better on GPUs, especially those with sufficient VRAM to hold the quantized model.\n",
       "*   **Limitations:** Very large models may still be challenging to run efficiently on low-end hardware, even with aggressive quantization.\n",
       "\n",
       "#### Comparison with Other Open-Source Solutions\n",
       "\n",
       "Llamma.cpp is not the only open-source LLM inference solution available. Other notable projects include:\n",
       "\n",
       "*   **Hugging Face Transformers:** A comprehensive library that provides tools for loading, running, and training various LLMs. While not as focused on extreme efficiency as Llamma.cpp, it offers a wider range of features and model support.\n",
       "*   **ONNX Runtime:** A cross-platform inference engine that supports a variety of hardware backends. It can be used to run LLMs in ONNX format.\n",
       "\n",
       "Llamma.cpp distinguishes itself with its focus on minimal dependencies, ease of use, and aggressive optimization for low-resource environments.\n",
       "\n",
       "### Comparative Analysis\n",
       "\n",
       "Groq, NVIDIA, and Llamma.cpp each offer distinct advantages and disadvantages for LLM inference, making their suitability highly dependent on the specific use case and priorities. This section provides a comparative analysis based on several key factors.\n",
       "\n",
       "#### Performance, Cost, Ease of Use, Accessibility, and Power Consumption\n",
       "\n",
       "When comparing performance, cost, ease of use, accessibility, and power consumption, the landscape is complex. Groq's LPU-based solution excels in low-latency inference, particularly for smaller models, due to its deterministic execution and high memory bandwidth. Benchmarks have shown Groq's LPU Inference Engine achieving impressive speeds, potentially outperforming NVIDIA GPUs in certain LLM inference tasks [2]. However, Groq's technology may face challenges with larger models and batch processing where GPUs traditionally shine.\n",
       "\n",
       "NVIDIA GPUs offer a more versatile solution, with a wide range of options catering to different performance and cost requirements. Their mature software ecosystem (CUDA, TensorRT) and extensive optimization techniques provide flexibility and high throughput, especially for large models and batched inference. However, GPUs can be power-hungry and may require significant capital investment.\n",
       "\n",
       "Llamma.cpp stands out as an accessible and cost-effective open-source solution, enabling LLM inference on commodity hardware. While it may not match the raw performance of Groq or NVIDIA, its ability to run on CPUs and lower-end GPUs makes it ideal for local development, research, and edge deployment where cost and accessibility are paramount. Llamma.cpp's quantization techniques allow for efficient inference even on resource-constrained devices, but this often comes with accuracy trade-offs.\n",
       "\n",
       "#### Strengths and Weaknesses in Different Use Cases\n",
       "\n",
       "Groq's strength lies in applications demanding ultra-low latency, such as real-time conversational AI or interactive applications. Its deterministic execution is also valuable in scenarios where predictable performance is critical. However, its limitations in software ecosystem maturity and power efficiency may hinder its adoption in certain areas.\n",
       "\n",
       "NVIDIA GPUs are well-suited for a broad range of use cases, from cloud deployment and high-throughput inference to research and development. Their versatility, extensive software support, and powerful hardware make them a popular choice for demanding LLM applications. However, the high cost of high-end GPUs can be a barrier to entry for some users.\n",
       "\n",
       "Llamma.cpp shines in edge computing scenarios, local development, and research projects where accessibility and cost are key considerations. Its ability to run on commodity hardware makes it ideal for prototyping, experimentation, and deployment on resource-constrained devices. However, its performance limitations may restrict its use in high-demand production environments.\n",
       "\n",
       "#### Impact of Market Concentration and Capital Investment on Bias\n",
       "\n",
       "The concentration of market power and capital investment in the hands of a few large players like NVIDIA can exacerbate bias in LLM inference. These companies have significant influence over the development and deployment of AI hardware and software, which can lead to biases being embedded in the technology. For example, if the training data used to optimize LLMs is biased, the resulting models will likely perpetuate those biases. Furthermore, the high cost of entry into the AI hardware market can limit competition and innovation, potentially leading to a lack of diversity in the types of AI systems that are developed and deployed. Open-source initiatives like Llamma.cpp can help to mitigate these biases by providing alternative platforms for LLM inference that are more accessible and transparent. However, these initiatives often lack the resources and capital investment of larger companies, which can limit their impact.\n",
       "\n",
       "### Future of LLM Inference\n",
       "\n",
       "The future of LLM inference is poised for significant advancements, driven by the increasing demand for faster, more efficient, and accessible AI. Emerging technologies and evolving market dynamics are shaping the landscape, pushing the boundaries of what's possible in LLM deployment and utilization.\n",
       "\n",
       "#### Emerging Technologies and Their Impact\n",
       "\n",
       "Several emerging technologies hold the potential to revolutionize LLM inference. New hardware architectures, such as neuromorphic computing and optical computing, promise significant improvements in energy efficiency and processing speed. Quantization techniques are also evolving, with research focusing on minimizing accuracy loss while maximizing compression. Model compression methods like pruning and distillation are becoming more sophisticated, enabling smaller, faster models suitable for edge deployment.\n",
       "\n",
       "#### Future Landscape of LLM Inference\n",
       "\n",
       "The future landscape of LLM inference hardware and software will likely be characterized by a diverse ecosystem catering to different user segments. High-performance cloud solutions will continue to dominate for large-scale deployments, while specialized edge devices will enable real-time inference in constrained environments. The balance between performance, cost, and power consumption will be a key consideration, with vendors offering tailored solutions for specific needs. We can expect to see more integration of LLM inference capabilities directly into consumer devices, such as smartphones and laptops.\n",
       "\n",
       "#### Evolution of LLM Accessibility\n",
       "\n",
       "LLM accessibility is expected to increase significantly over the next 3-5 years. Open-source initiatives like Llamma.cpp are democratizing access to LLMs, enabling individuals and smaller organizations to experiment with and deploy these models on commodity hardware. Cloud-based inference services are also becoming more affordable and user-friendly, lowering the barrier to entry for businesses of all sizes. As hardware and software become more efficient, LLMs will be increasingly accessible to a wider range of users, fostering innovation and driving adoption across various industries.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "In conclusion, the landscape of LLM inference is rapidly evolving, with diverse approaches vying for dominance. Groq's TSP architecture offers impressive low-latency inference, particularly for smaller models, but faces challenges in software ecosystem maturity and power efficiency. NVIDIA's GPUs, backed by a robust software ecosystem and extensive optimization libraries, remain a strong contender, offering a balance of performance and versatility. Llamma.cpp provides an accessible open-source solution, enabling LLM inference on commodity hardware with remarkable efficiency, albeit with trade-offs in accuracy and model size. The choice of platform depends heavily on the specific use case, budget, and performance requirements. As emerging technologies continue to advance, the future of LLM inference will likely be shaped by innovations in hardware architectures, quantization techniques, and model compression methods, ultimately driving greater accessibility and efficiency across various user segments.\n",
       "\n",
       "### References\n",
       "\n",
       "[0] https://medium.com/@laowang_journey/comparing-ai-hardware-architectures-sambanova-groq-cerebras-vs-nvidia-gpus-broadcom-asics-2327631c468e\n",
       "[1] https://www.nextplatform.com/2023/11/27/groq-says-it-can-deploy-1-million-ai-inference-chips-in-two-years/\n",
       "[2] https://www.reddit.com/r/LocalLLaMA/comments/1avz9hk/the_groq_chip_is_faster_than_nvidia_13x_faster/\n",
       "[3] https://promptengineering.org/groqs-lpu-advancing-llm-inference-efficiency/\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "article = results[name]['article']\n",
    "Markdown(article.replace(\"\\n#\", \"\\n##\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fbd092",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## LLM Inference Platforms: Groq, NVIDIA, Llama.cpp, and the Future\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Large Language Models (LLMs) have emerged as a transformative technology, demonstrating remarkable capabilities in natural language understanding and generation. However, deploying these models for real-time applications presents significant computational challenges. This section provides an overview of LLMs, defines LLM inference, and discusses the challenges associated with it.\n",
    "\n",
    "#### What are LLMs?\n",
    "\n",
    "LLMs are deep learning models with a large number of parameters, trained on vast amounts of text data. They are capable of performing a wide range of tasks, including text generation, translation, question answering, and code generation. Examples include GPT-3, LLaMA, and BERT.\n",
    "\n",
    "#### LLM Inference: Definition and Significance\n",
    "\n",
    "LLM inference refers to the process of using a trained LLM to generate outputs for new inputs. It is the deployment phase where the model is used to make predictions or generate text in real-world applications. Efficient LLM inference is crucial for enabling interactive applications and services powered by these models.\n",
    "\n",
    "#### Challenges in LLM Inference\n",
    "\n",
    "LLM inference faces several challenges, including high latency, limited throughput, and substantial memory requirements. The computational intensity of LLMs demands specialized hardware and software optimization techniques to achieve acceptable performance. Memory bandwidth and power consumption also pose significant constraints.\n",
    "\n",
    "### Groq and LLM Inference\n",
    "\n",
    "Groq offers a unique approach to LLM inference with its Tensor Streaming Processor (TSP) architecture. Unlike traditional GPUs, Groq's architecture emphasizes deterministic data flow and high memory bandwidth, which are particularly well-suited for the demands of LLM inference.\n",
    "\n",
    "#### Groq's Hardware Architecture\n",
    "\n",
    "Groq's hardware architecture is centered around the Tensor Streaming Processor (TSP), now referred to as the Language Processing Unit (LPU). The LPU is designed for high computational throughput and low latency, crucial for LLM inference. A key feature is its deterministic data flow, which allows for predictable performance and efficient resource utilization. The architecture prioritizes high memory bandwidth to keep the processing units fed with data, minimizing bottlenecks. However, each chip has limited on-chip memory (200MB), requiring racks to run larger LLMs.\n",
    "\n",
    "#### Groq's Software Stack and Optimization Techniques\n",
    "\n",
    "Groq's software stack includes a compiler that plays a critical role in scheduling and resource allocation. The compiler optimizes the execution of LLMs on the TSP architecture, taking advantage of its deterministic nature. The deterministic programming model simplifies optimization and allows for predictable performance. Groq's architecture doesn't get faster for batch sizes >1.\n",
    "\n",
    "#### Performance Benchmarks and Comparisons\n",
    "\n",
    "Groq has demonstrated impressive performance benchmarks, particularly in terms of latency. A single Groq LPU card can outperform expensive cloud GPU instances on LLM serving. Groq's architecture shines in scenarios where low latency is paramount.\n",
    "\n",
    "#### Power Efficiency and Thermal Characteristics\n",
    "\n",
    "Groq's architecture is designed for power efficiency. By minimizing data movement and maximizing resource utilization, the TSP-based systems can achieve competitive performance per watt. The reduced need for over-provisioning for batch size contributes to power savings.\n",
    "\n",
    "#### Handling Variability and Stochasticity\n",
    "\n",
    "Groq addresses the variability and stochasticity of LLM inference workloads through its scheduling and resource allocation mechanisms. The deterministic nature of the TSP architecture allows for precise control over execution, mitigating the impact of variability.\n",
    "\n",
    "### NVIDIA and LLM Inference\n",
    "\n",
    "NVIDIA's approach to LLM inference leverages its powerful GPUs and a comprehensive software ecosystem. Their strategy centers around maximizing parallelism and providing developers with robust tools for optimization.\n",
    "\n",
    "#### NVIDIA GPUs and LLM Inference\n",
    "\n",
    "NVIDIA GPUs, particularly architectures like H100 and A100, are designed for massively parallel computation, making them well-suited for the demands of LLM inference. These GPUs contain thousands of cores that can simultaneously perform the matrix multiplications and other operations that are fundamental to deep learning. The high memory bandwidth and large memory capacity of NVIDIA GPUs also allow them to handle large models and datasets efficiently.\n",
    "\n",
    "#### NVIDIA's Software Ecosystem and Optimization\n",
    "\n",
    "NVIDIA's software ecosystem, including CUDA and TensorRT, provides developers with the tools to optimize LLMs for inference. CUDA is a parallel computing platform and programming model that allows developers to harness the power of NVIDIA GPUs for general-purpose computing. TensorRT is an SDK for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning applications.\n",
    "\n",
    "The advantages of using CUDA and TensorRT include: broad hardware compatibility, mature software tools, and extensive community support. Challenges include: the complexity of CUDA programming and the need for specialized knowledge to optimize models for TensorRT.\n",
    "\n",
    "#### Performance Benchmarks and Comparisons\n",
    "\n",
    "NVIDIA GPUs offer high performance and scalability for LLM inference. Benchmarks consistently show that NVIDIA GPUs can deliver high throughput and low latency for a wide range of LLMs. However, power consumption can be a concern, especially for large models. The performance and power efficiency of NVIDIA GPUs vary depending on the specific model, the size of the LLM, and the optimization techniques used. According to a report on NextPlatform, when comparing against Groq's LPU, NVIDIA's GPUs can do an inference in one-tenth the time at one tenth the cost of generating the tokens. It takes the Nvidia GPU somewhere on the order of 10 joules to 30 joules to generate tokens in a response. [1]\n",
    "\n",
    "#### Software Development and Tooling\n",
    "\n",
    "Optimizing LLMs for inference on NVIDIA GPUs involves several steps, including: model quantization, graph optimization, and kernel fusion. NVIDIA provides a suite of tools and libraries to assist developers with these tasks. Key challenges include: balancing performance and accuracy, managing memory usage, and adapting to new model architectures. Advantages include: access to a wide range of pre-trained models and optimized kernels, and the ability to leverage NVIDIA's expertise in deep learning.\n",
    "\n",
    "#### Scalability\n",
    "\n",
    "NVIDIA's GPU architecture scales effectively using technologies like NVLink and NVSwitch for multi-GPU systems. NVLink provides high-bandwidth, low-latency interconnects between GPUs, allowing them to share data and coordinate computations efficiently. NVSwitch enables all-to-all communication between GPUs in a system. NVIDIA also offers mature software tools for distributed training and inference across clusters of GPUs, making it possible to scale LLM inference to handle large workloads.\n",
    "\n",
    "### Llamma.cpp and Open-Source LLM Inference\n",
    "\n",
    "Llamma.cpp has emerged as a significant player in the open-source LLM inference landscape, providing a platform for running large language models on a variety of hardware, including CPUs. It democratizes access to LLMs by enabling local inference, reducing reliance on proprietary hardware and cloud services.\n",
    "\n",
    "#### Introduction to Llamma.cpp\n",
    "\n",
    "Llamma.cpp is a project focused on enabling efficient inference of large language models (LLMs) using C++. Its primary goal is to make LLMs accessible to a wider audience by allowing them to run on consumer-grade hardware, including laptops and desktops, without requiring specialized GPUs. This is achieved through a combination of optimization techniques and support for various quantization methods.\n",
    "\n",
    "#### Architecture and Implementation\n",
    "\n",
    "Llamma.cpp is written in C++ for performance and portability. It leverages techniques like quantization (reducing the precision of weights and activations) to minimize memory footprint and computational requirements. The architecture is designed to be modular, allowing for easy integration of new models and hardware platforms. It supports various platforms, including x86, ARM, and Apple Silicon. The core implementation focuses on efficient matrix multiplication and other linear algebra operations crucial for LLM inference.\n",
    "\n",
    "#### Performance and Limitations\n",
    "\n",
    "Llamma.cpp's performance varies depending on the hardware and model used. While it may not match the throughput of high-end GPUs, it offers a viable option for local inference, especially for smaller models or when running on resource-constrained devices. Limitations include slower inference speeds compared to GPU-accelerated solutions and potential accuracy degradation due to quantization. However, ongoing development continuously improves its performance and expands its capabilities.\n",
    "\n",
    "#### Comparison with Other Open-Source Solutions\n",
    "\n",
    "Several other open-source LLM inference solutions exist, such as those based on ONNX Runtime or optimized for specific hardware architectures. Llamma.cpp distinguishes itself through its focus on simplicity, portability, and ease of use. It often serves as a reference implementation and a starting point for researchers and developers exploring LLM inference optimization techniques. Compared to some more complex solutions, Llamma.cpp is easier to set up and use, making it attractive to users with limited technical expertise.\n",
    "\n",
    "#### Quantization Techniques\n",
    "\n",
    "Llamma.cpp employs various quantization techniques to reduce the memory footprint and computational demands of LLMs. Quantization involves converting the floating-point weights and activations of a model to lower-precision integer representations (e.g., 8-bit or 4-bit integers). This reduces the model size and speeds up inference, but it can also lead to a loss of accuracy. The effectiveness and impact on accuracy of these quantization techniques can vary depending on the hardware platform. Groq's TSP, with its deterministic data flow, might exhibit different quantization behavior compared to NVIDIA GPUs, which rely on massively parallel architectures. Further research and experimentation are needed to fully understand these differences.\n",
    "\n",
    "### Comparative Analysis\n",
    "\n",
    "A comprehensive comparison of Groq, NVIDIA, and Llamma.cpp reveals distinct advantages and disadvantages across various parameters, including performance, cost, ease of use, and accessibility. Each platform caters to different needs and use cases, making a direct comparison complex but insightful.\n",
    "\n",
    "#### Performance, Cost, Ease of Use, and Accessibility\n",
    "\n",
    "When considering performance, Groq stands out for its low latency inference, particularly beneficial for real-time applications. NVIDIA GPUs, on the other hand, offer high throughput and scalability, making them suitable for large-scale deployments. Llamma.cpp provides a more accessible entry point for local inference, but its performance is generally lower compared to the other two, especially with larger models. In terms of cost, Llamma.cpp offers the most cost-effective solution as it leverages existing hardware. Groq's LPU cards can be expensive, but they offer a compelling performance per dollar for specific workloads. NVIDIA GPUs vary in price, with high-end models being a significant investment. Ease of use is another differentiating factor. NVIDIA boasts a mature software ecosystem with CUDA and TensorRT, providing extensive tools and libraries for developers. Llamma.cpp is relatively easy to set up and use, especially for those familiar with C++. Groq's software stack, while powerful, may require a steeper learning curve. Accessibility is also a key consideration. NVIDIA GPUs are widely available and supported by major cloud providers. Llamma.cpp can be run on a wide range of hardware, making it highly accessible. Groq's hardware is less readily available, potentially limiting its accessibility.\n",
    "\n",
    "#### Strengths and Weaknesses in Different Use Cases\n",
    "\n",
    "Groq excels in use cases demanding ultra-low latency, such as real-time language translation or interactive AI applications. Its deterministic data flow and high memory bandwidth make it well-suited for these scenarios. NVIDIA GPUs shine in applications requiring high throughput and scalability, such as large-scale language model serving or training. Their massively parallel architecture and mature software ecosystem provide the necessary tools for these tasks. Llamma.cpp is ideal for local inference and experimentation, allowing users to run LLMs on their own hardware without relying on cloud services. However, it may not be suitable for production environments with high performance requirements. A key weakness of Groq is the limited on-chip memory, necessitating racks of chips to run larger LLMs. NVIDIA's weakness includes underutilization if batch sizes are not optimized, leading to wasted power. Llamma.cpp's primary weakness is its performance ceiling compared to dedicated hardware solutions.\n",
    "\n",
    "#### Power Efficiency Comparison\n",
    "\n",
    "Groq's LPU is designed for power efficiency, particularly when running at its optimal batch size, avoiding the over-provisioning issues that can plague GPU deployments. NVIDIA GPUs, while powerful, can consume significant power, especially when not fully utilized. Llamma.cpp's power consumption depends on the underlying hardware, but it generally consumes less power than dedicated GPU or LPU solutions. The key architectural factors contributing to Groq's power efficiency include its deterministic data flow and elimination of external memory access bottlenecks. By minimizing data movement and maximizing on-chip computation, Groq reduces energy consumption. NVIDIA GPUs, on the other hand, rely on a more traditional architecture with external memory access, which can be a significant source of power consumption.\n",
    "\n",
    "#### Software Stacks and Development Tools\n",
    "\n",
    "NVIDIA's CUDA and TensorRT provide a rich set of tools and libraries for optimizing LLMs, along with extensive community support and a wide range of pre-trained models. Groq's software stack, while offering a deterministic programming model, may have a smaller community and fewer pre-trained models readily available. This can increase the initial effort required to deploy and optimize LLMs on Groq's platform. Llamma.cpp benefits from its simplicity and ease of use, but it may lack the advanced optimization features and extensive tooling available in NVIDIA's ecosystem. The choice of software stack and development tools depends on the specific requirements of the project, the available expertise, and the desired level of control over the optimization process.\n",
    "\n",
    "### Future of LLM Inference\n",
    "\n",
    "The field of LLM inference is rapidly evolving, driven by the increasing demand for faster, more efficient, and more accessible AI. Several emerging technologies and trends are poised to significantly impact the future of LLM inference, shaping the landscape for both hardware and software solutions.\n",
    "\n",
    "#### Emerging Technologies and Their Impact\n",
    "\n",
    "Several emerging technologies promise to revolutionize LLM inference.  **Specialized Hardware:**  Continued development of specialized hardware like Groq's LPU and other ASICs (Application-Specific Integrated Circuits) will likely lead to further performance gains and power efficiency improvements.  **Quantization and Pruning:** Advances in model compression techniques like quantization (reducing the precision of weights) and pruning (removing less important connections) will enable smaller, faster models that require less memory and compute.  **Novel Architectures:** Exploration of novel neural network architectures, such as Mixture of Experts (MoE), could lead to more efficient and scalable models.  **Near-Memory Computing:** Architectures that bring computation closer to memory can reduce data movement bottlenecks, a major factor in LLM inference latency.\n",
    "\n",
    "#### Future Landscape and Predictions\n",
    "\n",
    "The future of LLM inference is likely to be characterized by a few key trends.  **Increased Specialization:**  We can expect to see further specialization of hardware and software for LLM inference, with solutions tailored to specific model sizes, architectures, and deployment scenarios.  **Edge Inference:**  The ability to run LLMs on edge devices (e.g., smartphones, IoT devices) will become increasingly important, enabling real-time AI applications with low latency and enhanced privacy.  **Democratization of AI:**  Open-source solutions like Llama.cpp will continue to play a crucial role in democratizing access to LLMs, allowing individuals and smaller organizations to experiment with and deploy these models without relying on expensive proprietary platforms.  **Cloud vs. On-Premise:**  The balance between cloud-based and on-premise LLM inference will depend on factors such as cost, latency requirements, data security concerns, and regulatory constraints.  It's likely that both deployment models will coexist, with cloud solutions being favored for large-scale, general-purpose applications and on-premise solutions being preferred for latency-sensitive or data-private use cases.\n",
    "\n",
    "#### Ethical and Environmental Implications\n",
    "\n",
    "The increasing use of LLMs raises important ethical and environmental considerations.  **Energy Consumption:**  LLM inference can be energy-intensive, particularly when using large models and running them at scale. Groq's architecture, with its focus on efficiency, offers a potential advantage in reducing the carbon footprint of LLM deployments compared to traditional GPU-based solutions. Open-source solutions like Llama.cpp can also contribute to sustainability by enabling inference on less powerful hardware.  **Bias and Fairness:**  LLMs can perpetuate and amplify biases present in their training data, leading to unfair or discriminatory outcomes. It is crucial to carefully evaluate and mitigate these biases to ensure that LLMs are used responsibly.  **Accessibility:**  The cost and complexity of LLM inference can create barriers to access, potentially exacerbating existing inequalities. Efforts to democratize access to LLMs through open-source solutions and more efficient hardware are essential to ensure that the benefits of AI are shared broadly.\n",
    "\n",
    "#### Security Implications\n",
    "\n",
    "Security is a paramount concern in LLM inference.  **Malicious Code Injection:**  LLMs are vulnerable to prompt injection attacks, where malicious actors can manipulate the model's behavior by crafting carefully designed prompts. Robust input validation and sanitization techniques are needed to mitigate this risk.  **Data Breaches:**  LLMs can inadvertently leak sensitive information if they are not properly secured. Access control mechanisms, data encryption, and privacy-preserving techniques are essential to protect confidential data.  **Model Tampering:**  Adversaries could attempt to tamper with the LLM itself, either by modifying its weights or by injecting malicious code. Model integrity checks and secure deployment practices are needed to prevent such attacks. The choice of platform can also impact security. Open-source solutions like Llama.cpp offer greater transparency and control, but they also require more expertise to secure properly. Proprietary platforms like Groq and NVIDIA may offer more robust security features, but they also involve a greater degree of trust in the vendor.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In conclusion, the landscape of LLM inference is rapidly evolving, with diverse platforms like Groq, NVIDIA, and Llamma.cpp offering distinct advantages and disadvantages. Groq's LPU-based architecture excels in low-latency inference due to its deterministic data flow and high memory bandwidth, making it suitable for real-time applications. NVIDIA's GPUs, with their massively parallel architecture and mature software ecosystem, provide high throughput and scalability, catering to large-scale deployments. Llamma.cpp, as an open-source solution, offers flexibility and accessibility, empowering researchers and developers to experiment with LLM inference on commodity hardware.\n",
    "\n",
    "The choice of platform depends heavily on the specific use case, budget, and performance requirements. Groq shines in latency-sensitive applications, while NVIDIA dominates in throughput-demanding scenarios. Llamma.cpp provides a cost-effective and customizable solution for smaller-scale deployments and research purposes.\n",
    "\n",
    "Looking ahead, emerging technologies like specialized AI accelerators and advanced quantization techniques promise to further optimize LLM inference. The future landscape will likely be shaped by a combination of hardware and software innovations, with a focus on improving efficiency, reducing costs, and expanding accessibility. Ethical and environmental considerations will also play a crucial role in shaping the development and deployment of LLM inference platforms, encouraging the adoption of energy-efficient solutions and responsible AI practices. Security considerations around malicious code injection and data breaches will also continue to be paramount.\n",
    "\n",
    "Ultimately, the ongoing advancements in LLM inference will pave the way for more widespread adoption of AI-powered applications across various industries, transforming how we interact with technology and solve complex problems.\n",
    "\n",
    "### References\n",
    "\n",
    "[1]  \"Groq Says It Can Deploy 1 Million AI Inference Chips in Two Years.\" *The Next Platform*, 27 Nov. 2023, https://www.nextplatform.com/2023/11/27/groq-says-it-can-deploy-1-million-ai-inference-chips-in-two-years/.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2fba03",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
