{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8369ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_topic = \"Impact of millon-plus token context window language models on RAG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f12a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1fa9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelatedSubject(topics=['Large language models', 'Retrieval augmented generation', 'Context window', 'Natural language processing', 'Artificial intelligence', 'Machine learning', 'Deep learning', 'Transformer networks', 'Wikipedia'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "fast_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.0,)\n",
    "long_context_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "    temperature=0.0)\n",
    "\n",
    "gen_related_topics_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"I'm writing a Wikipedia page for a topic mentionde below. Please identify and recomend some Wikipedia pagens on clasely related topics\n",
    "    \n",
    "    Please list the as many subject and urls as you can\n",
    "    \n",
    "    Topic of interest: {topic}\"\"\"\n",
    ")\n",
    "\n",
    "class RelatedSubject(BaseModel):\n",
    "    topics: List[str] = Field(\n",
    "    description=\"Comprehensive list of related subjects as background research\",\n",
    "    )\n",
    "\n",
    "expand_chain = gen_related_topics_prompt | fast_llm.with_structured_output(RelatedSubject)\n",
    "\n",
    "related_subjects = await expand_chain.ainvoke({'topic': example_topic})\n",
    "related_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f317fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\petro_m\\AppData\\Local\\Temp\\ipykernel_6980\\833876501.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  perspectives.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'editors': [{'affiliation': 'Independent AI Researcher',\n",
       "   'name': 'Dr. Anya Sharma',\n",
       "   'role': 'Performance Analyst',\n",
       "   'description': 'Focuses on the quantitative analysis of RAG performance with large context windows, including metrics like recall, precision, and latency. Concerned with benchmarking different models and retrieval strategies.'},\n",
       "  {'affiliation': 'Large Language Model Developer',\n",
       "   'name': 'Dr. Kenji Tanaka',\n",
       "   'role': 'Model Architect',\n",
       "   'description': 'Interested in the architectural modifications and training techniques required to effectively utilize million-plus token context windows. Focuses on model efficiency and scalability.'},\n",
       "  {'affiliation': 'Enterprise Knowledge Management',\n",
       "   'name': 'Ms. Ingrid Olsen',\n",
       "   'role': 'Enterprise Application Specialist',\n",
       "   'description': 'Concerned with the practical applications of large context RAG in enterprise settings, including knowledge retrieval, document summarization, and question answering. Focuses on data security and access control.'},\n",
       "  {'affiliation': 'AI Ethics Research Institute',\n",
       "   'name': 'Dr. Omar Hassan',\n",
       "   'role': 'AI Ethics Researcher',\n",
       "   'description': 'Focuses on the ethical implications of large context RAG, including bias amplification, misinformation, and the potential for misuse. Concerned with developing mitigation strategies and ethical guidelines.'},\n",
       "  {'affiliation': 'Open Source RAG Framework Developer',\n",
       "   'name': 'Mr. Jian Li',\n",
       "   'role': 'Open Source Advocate',\n",
       "   'description': 'Interested in building open-source tools and libraries to facilitate the development and deployment of large context RAG systems. Focuses on ease of use and community contributions.'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.runnables import RunnableLambda, chain as as_runnable\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)\n",
    "\n",
    "def format_doc(doc, max_length=1000):\n",
    "    related = \"- \".join(doc.metadata['categories'])\n",
    "    return f\"### {doc.metadata['title']}\\n\\nSummary: {doc.page_content}\\nRelated: {related}\\n\\n\"[:max_length]\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([format_doc(doc) for doc in docs])\n",
    "\n",
    "\n",
    "class Editor(BaseModel):\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the editor\"\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"Name of the editor\",\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"Role of the editor in the context of the topic.\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Description of the editor's focus, concers, and motives`\"\n",
    "    )\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\"\n",
    "    \n",
    "class Perspectives(BaseModel):\n",
    "    editors: List[Editor] = Field(\n",
    "        description=\"List of editors with their perspectives on the topic\"\n",
    "    )\n",
    "\n",
    "gen_perspectives_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "         \"\"\"You need to select a diverse(and distinc) group of Wikipedia editors who will work together to create a comprehensive article on the topic.\n",
    "         You can use other Wikipedia pages of related topics for inspiration. For each editor, add description of what they will focus on.\n",
    "         \n",
    "         Wiki page outlines of related topics for inspiration: \n",
    "         {examples}\"\"\"),\n",
    "         (\"user\",\"Topic of interest: {topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_perspectives_chain = gen_perspectives_prompt | ChatGoogleGenerativeAI(model='gemini-2.0-flash').with_structured_output(Perspectives)\n",
    "\n",
    "@as_runnable\n",
    "async def survey_subjects(topics: str):\n",
    "    reletaed_subjects = await expand_chain.ainvoke({'topic': topics})\n",
    "    retrieved_docs = await wikipedia_retriever.abatch(reletaed_subjects.topics, return_exceptions=True)\n",
    "    all_docs = []\n",
    "    for docs in retrieved_docs:\n",
    "        if isinstance(docs, BaseException):\n",
    "            continue\n",
    "        all_docs.extend(docs)\n",
    "    formatted = format_docs(all_docs)\n",
    "    return await gen_perspectives_chain.ainvoke({\n",
    "        \"examples\": formatted,\n",
    "        \"topic\": topics\n",
    "    })\n",
    "\n",
    "perspectives = await survey_subjects.ainvoke(example_topic)\n",
    "perspectives.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adee1233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affiliation='Independent AI Researcher' name='Dr. Anya Sharma' role='Performance Analyst' description='Focuses on the quantitative analysis of RAG performance with large context windows, including metrics like recall, precision, and latency. Concerned with benchmarking different models and retrieval strategies.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Dr. Sharma, it's a pleasure to speak with you.  My name is Alex, and I'm a Wikipedia editor working on an article about the impact of million-plus token context window language models on Retrieval Augmented Generation (RAG).  My focus is on the practical implications and limitations of these models for real-world applications.  My first question is:  What are some of the most significant challenges you've encountered in benchmarking the recall and precision of RAG systems using these extremely large context windows, beyond the obvious computational constraints?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage, AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "from typing import Annotated, Sequence\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "def add_messages(left, right):\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    return left+right\n",
    "\n",
    "def update_references(references, new_references):\n",
    "    if not references:\n",
    "        references = {}\n",
    "    references.update(new_references)\n",
    "    return references\n",
    "\n",
    "def update_editor(editor, new_editor):\n",
    "    # Can only set at the outset\n",
    "    if not editor:\n",
    "        return new_editor\n",
    "    return editor\n",
    "\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    references: Annotated[Optional[dict], update_references]\n",
    "    editor: Annotated[Optional[Editor], update_editor]\n",
    "\n",
    "\n",
    "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experienced Wikipedia writer and want to edit a specific page. \\\n",
    "Besides your identity as a Wikipedia writer, you have a specific focus when researching the topic. \\\n",
    "Now, you are chatting with an expert to get information. Ask good questions to get more useful information.\n",
    "\n",
    "When you have no more questions to ask, say \"Thank you so much for your help!\" to end the conversation.\\\n",
    "Please only ask one question at a time and don't ask what you have asked before.\\\n",
    "Your questions should be related to the topic you want to write.\n",
    "Be comprehensive and curious, gaining as much unique insight from the expert as possible.\\\n",
    "\n",
    "Stay true to your specific perspective:\n",
    "\n",
    "{persona}\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def tag_with_name(ai_message: AIMessage, name:str):\n",
    "    ai_message.name = name\n",
    "    return ai_message\n",
    "\n",
    "\n",
    "def swap_roles(state: InterviewState, name):\n",
    "    converted = []\n",
    "    for message in state['messages']:\n",
    "        if isinstance(message, AIMessage) and message.name != name:\n",
    "            message = HumanMessage(**message.dict(exclude=(\"type\")))\n",
    "        converted.append(message)\n",
    "    return {'messages': converted}\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def generate_question(state:InterviewState):\n",
    "    editor = state['editor']\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=editor.name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | fast_llm\n",
    "        | RunnableLambda(tag_with_name).bind(name=editor.name)\n",
    "    )\n",
    "    result = await gn_chain.ainvoke(state)\n",
    "    return {\"messages\": [result]}\n",
    "\n",
    "\n",
    "print(perspectives.editors[0])\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(f\"So you said you were wrting an article on {example_topic}\")\n",
    "]\n",
    "\n",
    "question = await generate_question.ainvoke(\n",
    "    {'editor':perspectives.editors[0],\n",
    "     \"messages\":messages}\n",
    ")\n",
    "\n",
    "question['messages'][0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85762a1",
   "metadata": {},
   "source": [
    "## Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15da617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: ['challenges in benchmarking RAG systems with large context windows recall precision']\n"
     ]
    }
   ],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(\n",
    "        description=\"Comprehensive list of search engine queries to answer the user's questons.\"\n",
    "\n",
    "    )\n",
    "\n",
    "gen_queries_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            \"You are a helpful research assistant. Query the search engine to answer the user's questions.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True)\n",
    "    ]\n",
    ")\n",
    "gen_queries_chain = gen_queries_prompt | ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\").with_structured_output(Queries, include_raw=True)\n",
    "\n",
    "queries = await gen_queries_chain.ainvoke({\n",
    "    'messages': [HumanMessage(content=question['messages'][0].content)]\n",
    "})\n",
    "print(f\"Queries: {queries['parsed'].queries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cb418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
