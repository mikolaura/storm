{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8369ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_topic = \"Impact of millon-plus token context window language models on RAG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f12a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db1fa9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelatedSubject(topics=['Large language models', 'Retrieval augmented generation', 'Context window', 'Natural language processing', 'Artificial intelligence', 'Machine learning', 'Deep learning', 'Transformer networks', 'Wikipedia'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "fast_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.0,)\n",
    "long_context_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "    temperature=0.0)\n",
    "\n",
    "gen_related_topics_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"I'm writing a Wikipedia page for a topic mentionde below. Please identify and recomend some Wikipedia pagens on clasely related topics\n",
    "    \n",
    "    Please list the as many subject and urls as you can\n",
    "    \n",
    "    Topic of interest: {topic}\"\"\"\n",
    ")\n",
    "\n",
    "class RelatedSubject(BaseModel):\n",
    "    topics: List[str] = Field(\n",
    "    description=\"Comprehensive list of related subjects as background research\",\n",
    "    )\n",
    "\n",
    "expand_chain = gen_related_topics_prompt | fast_llm.with_structured_output(RelatedSubject)\n",
    "\n",
    "related_subjects = await expand_chain.ainvoke({'topic': example_topic})\n",
    "related_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f317fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20164\\833876501.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  perspectives.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'editors': [{'affiliation': 'Independent AI Researcher',\n",
       "   'name': 'Dr. Anya Sharma',\n",
       "   'role': 'Technical Accuracy Reviewer',\n",
       "   'description': 'Focuses on the practical implications of large context windows for RAG, particularly concerning information retrieval accuracy and the reduction of hallucination.'},\n",
       "  {'affiliation': 'Enterprise Solutions Architect',\n",
       "   'name': 'Kenji Tanaka',\n",
       "   'role': 'Scalability and Cost Analyst',\n",
       "   'description': 'Concerned with the scalability and cost-effectiveness of using million-plus token context windows in enterprise RAG applications. Focuses on optimizing infrastructure and reducing latency.'},\n",
       "  {'affiliation': 'NLP Ethicist',\n",
       "   'name': 'Fatima Hassan',\n",
       "   'role': 'Ethics and Bias Auditor',\n",
       "   'description': 'Examines the ethical considerations of using large context windows, including potential biases amplified by the increased data processing and the impact on user privacy.'},\n",
       "  {'affiliation': 'Academic Linguist',\n",
       "   'name': 'Professor David Chen',\n",
       "   'role': 'Linguistic Analysis',\n",
       "   'description': \"Analyzes how large context windows affect the models' understanding of nuanced language, long-range dependencies, and discourse coherence in RAG applications.\"},\n",
       "  {'affiliation': 'Information Security Consultant',\n",
       "   'name': 'Isabelle Dubois',\n",
       "   'role': 'Security Vulnerability Assessor',\n",
       "   'description': 'Focuses on the security vulnerabilities introduced by processing extremely large contexts, including potential for prompt injection attacks and data exfiltration in RAG systems.'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.runnables import RunnableLambda, chain as as_runnable\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)\n",
    "\n",
    "def format_doc(doc, max_length=1000):\n",
    "    related = \"- \".join(doc.metadata['categories'])\n",
    "    return f\"### {doc.metadata['title']}\\n\\nSummary: {doc.page_content}\\nRelated: {related}\\n\\n\"[:max_length]\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([format_doc(doc) for doc in docs])\n",
    "\n",
    "\n",
    "class Editor(BaseModel):\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the editor\"\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"Name of the editor\",\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"Role of the editor in the context of the topic.\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Description of the editor's focus, concers, and motives`\"\n",
    "    )\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\"\n",
    "    \n",
    "class Perspectives(BaseModel):\n",
    "    editors: List[Editor] = Field(\n",
    "        description=\"List of editors with their perspectives on the topic\"\n",
    "    )\n",
    "\n",
    "gen_perspectives_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "         \"\"\"You need to select a diverse(and distinc) group of Wikipedia editors who will work together to create a comprehensive article on the topic.\n",
    "         You can use other Wikipedia pages of related topics for inspiration. For each editor, add description of what they will focus on.\n",
    "         \n",
    "         Wiki page outlines of related topics for inspiration: \n",
    "         {examples}\"\"\"),\n",
    "         (\"user\",\"Topic of interest: {topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_perspectives_chain = gen_perspectives_prompt | ChatGoogleGenerativeAI(model='gemini-2.0-flash').with_structured_output(Perspectives)\n",
    "\n",
    "@as_runnable\n",
    "async def survey_subjects(topics: str):\n",
    "    reletaed_subjects = await expand_chain.ainvoke({'topic': topics})\n",
    "    retrieved_docs = await wikipedia_retriever.abatch(reletaed_subjects.topics, return_exceptions=True)\n",
    "    all_docs = []\n",
    "    for docs in retrieved_docs:\n",
    "        if isinstance(docs, BaseException):\n",
    "            continue\n",
    "        all_docs.extend(docs)\n",
    "    formatted = format_docs(all_docs)\n",
    "    return await gen_perspectives_chain.ainvoke({\n",
    "        \"examples\": formatted,\n",
    "        \"topic\": topics\n",
    "    })\n",
    "\n",
    "perspectives = await survey_subjects.ainvoke(example_topic)\n",
    "perspectives.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adee1233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affiliation='Independent AI Researcher' name='Dr. Anya Sharma' role='Technical Accuracy Reviewer' description='Focuses on the practical implications of large context windows for RAG, particularly concerning information retrieval accuracy and the reduction of hallucination.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Dr. Sharma, it's a pleasure to speak with you.  My name is Dr. Anya Sharma, and I'm working on a Wikipedia article about the impact of million-plus token context window language models on Retrieval Augmented Generation (RAG). My focus is on the practical implications for information retrieval accuracy and hallucination reduction.  My first question is:  What are some of the most significant challenges you've encountered in deploying million-plus token context window models within a RAG pipeline, specifically concerning the management and processing of such large contexts?\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage, AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "from typing import Annotated, Sequence\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "def add_messages(left, right):\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    return left+right\n",
    "\n",
    "def update_references(references, new_references):\n",
    "    if not references:\n",
    "        references = {}\n",
    "    references.update(new_references)\n",
    "    return references\n",
    "\n",
    "def update_editor(editor, new_editor):\n",
    "    # Can only set at the outset\n",
    "    if not editor:\n",
    "        return new_editor\n",
    "    return editor\n",
    "\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    references: Annotated[Optional[dict], update_references]\n",
    "    editor: Annotated[Optional[Editor], update_editor]\n",
    "\n",
    "\n",
    "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experienced Wikipedia writer and want to edit a specific page. \\\n",
    "Besides your identity as a Wikipedia writer, you have a specific focus when researching the topic. \\\n",
    "Now, you are chatting with an expert to get information. Ask good questions to get more useful information.\n",
    "\n",
    "When you have no more questions to ask, say \"Thank you so much for your help!\" to end the conversation.\\\n",
    "Please only ask one question at a time and don't ask what you have asked before.\\\n",
    "Your questions should be related to the topic you want to write.\n",
    "Be comprehensive and curious, gaining as much unique insight from the expert as possible.\\\n",
    "\n",
    "Stay true to your specific perspective:\n",
    "\n",
    "{persona}\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def tag_with_name(ai_message: AIMessage, name:str):\n",
    "    ai_message.name = name\n",
    "    return ai_message\n",
    "\n",
    "\n",
    "def swap_roles(state: InterviewState, name):\n",
    "    converted = []\n",
    "    for message in state['messages']:\n",
    "        if isinstance(message, AIMessage) and message.name != name:\n",
    "            message = HumanMessage(**message.model_dump(exclude=(\"type\")))\n",
    "        converted.append(message)\n",
    "    return {'messages': converted}\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def generate_question(state:InterviewState):\n",
    "    editor = state['editor']\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=editor.name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | fast_llm\n",
    "        | RunnableLambda(tag_with_name).bind(name=editor.name)\n",
    "    )\n",
    "    result = await gn_chain.ainvoke(state)\n",
    "    return {\"messages\": [result]}\n",
    "\n",
    "\n",
    "print(perspectives.editors[0])\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(f\"So you said you were wrting an article on {example_topic}\")\n",
    "]\n",
    "\n",
    "question = await generate_question.ainvoke(\n",
    "    {'editor':perspectives.editors[0],\n",
    "     \"messages\":messages}\n",
    ")\n",
    "\n",
    "question['messages'][0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85762a1",
   "metadata": {},
   "source": [
    "## Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15da617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: ['challenges of deploying million-plus token context window models in RAG pipeline']\n"
     ]
    }
   ],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(\n",
    "        description=\"Comprehensive list of search engine queries to answer the user's questons.\"\n",
    "\n",
    "    )\n",
    "\n",
    "gen_queries_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            \"You are a helpful research assistant. Query the search engine to answer the user's questions.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True)\n",
    "    ]\n",
    ")\n",
    "gen_queries_chain = gen_queries_prompt | ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\").with_structured_output(Queries, include_raw=True)\n",
    "\n",
    "queries = await gen_queries_chain.ainvoke({\n",
    "    'messages': [HumanMessage(content=question['messages'][0].content)]\n",
    "})\n",
    "print(f\"Queries: {queries['parsed'].queries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cb418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most significant challenges in deploying million-plus token context window models within a RAG pipeline center around the management and processing of such extensive contexts.  These challenges include the need for significant architectural changes and specialized training data to efficiently handle the increased data volume.  Furthermore, the expanded context window amplifies security risks, particularly when dealing with sensitive or proprietary information.  Debugging and tracing the source of information also becomes significantly more complex, making it difficult to pinpoint the origin of errors or unexpected outputs.  Finally, the computational resources required to process such large contexts are substantial, leading to increased latency and higher operational costs.\\n\\nCitations:\\n\\n[1]: https://aiagentslist.com/blog/is-rag-still-relevant-with-million-tokens-llms\\n[2]: https://arxiv.org/html/2503.00353v1\\n[3]: https://fabrity.com/blog/will-large-context-windows-kill-rag-pipelines/\\n[4]: https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "class AnswerWithCitations(BaseModel):\n",
    "    answer: str = Field(\n",
    "        description=\"Comprehensive answer to the user's question with citations.\"\n",
    "    )\n",
    "    cited_urls:List[str] = Field(\n",
    "        description=\"List of urls cited in the answer\"\n",
    "    )\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"{self.answer}\\n\\nCitations:\\n\\n\" + \"\\n\".join(f\"[{i+1}]: {url}\" for i, url in enumerate(self.cited_urls))\n",
    "    \n",
    "\n",
    "gen_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert who can use information effectively. You are chatting with a Wikipedia writer who wants\\\n",
    " to write a Wikipedia page on the topic you know. You have gathered the related information and will now use the information to form a response.\n",
    "\n",
    "Make your response as informative as possible and make sure every sentence is supported by the gathered information.\n",
    "Each response must be backed up by a citation from a reliable source, formatted as a footnote, reproducing the URLS after your response.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_answer_chain = gen_answer_prompt | fast_llm.with_structured_output(\n",
    "    AnswerWithCitations, include_raw=True\n",
    ").with_config(rum_name=\"GenerateAnswer\")\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=4)\n",
    "\n",
    "@tool\n",
    "async def search_engine(query:str):\n",
    "    \"\"\"Search engine to the internet\"\"\"\n",
    "    results = tavily_search.invoke(query)\n",
    "    return [{'content': r['content'], \"url\": r[\"url\"]} for r in results]\n",
    "\n",
    "\n",
    "async def gen_answer(\n",
    "        state: InterviewState,\n",
    "        config: Optional[RunnableConfig] =None,\n",
    "        name: str = \"Subject Matter Expert\",\n",
    "        max_str_len: int = 15000\n",
    "):\n",
    "    \n",
    "    swapped_state = swap_roles(state, name)\n",
    "    queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "    query_result = await search_engine.abatch(\n",
    "        queries['parsed'].queries, config, return_exceptions=True\n",
    "    )\n",
    "    successfull_result = [\n",
    "        res for res in query_result if not isinstance(res, Exception)\n",
    "        ]\n",
    "    all_query_results = {\n",
    "        res['url']: res['content'] for results in successfull_result for res in results\n",
    "    }\n",
    "    dumped = json.dumps(all_query_results)[:max_str_len]\n",
    "    ai_message: AIMessage = queries['raw']\n",
    "    tool_call = queries['raw'].tool_calls[0]\n",
    "    tool_id = tool_call['id']\n",
    "    tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "    swapped_state['messages'].extend([ai_message, tool_message])\n",
    "\n",
    "    generated = await gen_answer_chain.ainvoke(swapped_state)\n",
    "    cited_urls = set(generated['parsed'].cited_urls)\n",
    "    cited_references = {k:v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    formatted_message = AIMessage(name=name, content=generated['parsed'].as_str)\n",
    "    return {\"messages\": [formatted_message], \"references\": cited_references}\n",
    "\n",
    "\n",
    "example_answer = await gen_answer(\n",
    "    {\"messages\": [HumanMessage(content=question['messages'][0].content)]},\n",
    ")\n",
    "\n",
    "\n",
    "example_answer['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "223b2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_turns = 5\n",
    "\n",
    "\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"Subject_Matter_Expert\"):\n",
    "    messages = state[\"messages\"]\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "    if num_responses >= max_num_turns:\n",
    "        return END\n",
    "    last_question = messages[-2]\n",
    "    if last_question.content.endswith(\"Thank you so much for your help!\"):\n",
    "        return END\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "builder = StateGraph(InterviewState)\n",
    "\n",
    "builder.add_node(\"ask_question\", generate_question)\n",
    "builder.add_node(\"answer_question\", gen_answer)\n",
    "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
    "builder.add_edge(\"ask_question\", \"answer_question\")\n",
    "\n",
    "builder.set_entry_point(\"ask_question\")\n",
    "interview_graph = builder.compile().with_config(run_name=\"Conduct Interviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbdd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  {'ask_question': {'messages': [AIMessage(content=\"Hello Dr. Sharma, it's a pleasure to speak with you.  I'm writing a Wikipedia article on the impact of million-plus token context window language models on Retrieval Augmented Generation (RAG) systems, specifically focusing on how increased context windows affect the accuracy and hallucination rates of these systems. My first question is:  What are some of the most significant, empirically observed improvements in information retrieval accuracy that you've seen in RAG systems as a result of using million-plus token context windows, and what datasets or benchmarks were used to demonstrate these improvements?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, name='Dr. Anya Sharma', id='run--2fca0290-3aae-451f-b0d3-8b6dd0ef88d0-0', usage_metadata={'input_tokens': 204, 'output_tokens': 116, 'total_tokens': 320, 'input_token_details': {'cache_read': 0}})]}}\n",
      "--  {'answer_question': {'messages': [AIMessage(content=\"Studies show that million-plus token context windows in language models lead to significant improvements in information retrieval accuracy within RAG systems.  For example, one study using the Natural Questions dataset observed that models like Gemini 1.5 Pro achieved answer correctness values above 0.85 with a 2 million token context window.<sup>1</sup> Other research indicates substantial relative gains in retrieval-based RAG systems, with improvements exceeding 800% on benchmarks like HotpotQA using techniques like query decomposition.<sup>2</sup>  However, the impact isn't universally positive;  accuracy improvements are not always monotonic, and some models show peak performance at lower token counts.<sup>1</sup>  Furthermore,  datasets like FinanceBench, which involve longer documents, require larger context windows to achieve optimal recall.<sup>3</sup>  The effectiveness of increased context windows also depends on the specific RAG system and dataset used, with some studies showing that even with million-token windows, RAG remains a valuable component for maintaining accuracy and efficiency.<sup>4</sup>\\n\\nCitations:\\n\\n[1]: https://arxiv.org/html/2411.03538v1\\n[2]: https://arxiv.org/html/2506.00054v1\\n[3]: https://www.databricks.com/blog/long-context-rag-performance-llms\\n[4]: https://aiagentslist.com/blog/is-rag-still-relevant-with-million-tokens-llms\", additional_kwargs={}, response_metadata={}, name='Subject Matter Expert')], 'references': {'https://arxiv.org/html/2411.03538v1': 'Google Gemini 1.5 Pro (2 million tokens) [[5](https://arxiv.org/html/2411.03538v1#bib.bib5)] have led to speculation about whether long context models might eventually subsume traditional RAG workflows entirely. In this study, we empirically investigate the impact of increased context length on RAG performance and explore the limitations and challenges that arise in long context scenarios. [...] We note that we did not include any queries that failed in this way (i.e. by filtering) in the final accuracy score. On Natural Questions specifically, Gemini 1.5 Pro and Flash did remarkably well with answer correctness values above 0.85 at 2 million tokens context length (see Fig. [S2](https://arxiv.org/html/2411.03538v1#A2.F2 \"Figure S2 ‣ Appendix B Dataset Details ‣ Long Context RAG Performance of Large Language Models\")). [...] Performance of Large Language Models\")). Overall, we found that the following models show consistent accuracy improvement up to 100k tokens: o1-preview and o1-mini, GPT-4o and GPT-4o mini, Claude 3.5 Sonnet, Claude 3 Opus, and Gemini 1.5 Pro. These models exhibit largely monotonic behavior where the results don’t get significantly worse after they peak.', 'https://arxiv.org/html/2506.00054v1': 'Among retrieval-based RAG systems, models such as RQ-RAG, RankRAG, LQR, and LongRAG demonstrate substantial relative gains. Notably, RQ-RAG achieves over an 800% improvement from its raw LLM baseline on HotpotQA(Yang et al., [2018](https://arxiv.org/html/2506.00054v1#bib.bib82)), and a 275% improvement over standard retrieval, highlighting the effectiveness of sophisticated query decomposition techniques in multi-hop settings. Similarly, LQR achieves a 292% improvement from the raw baseline and [...] Among generator-based RAG systems primarily optimized for accuracy, SELF-RAG consistently demonstrates substantial gains across multiple datasets. It achieves over a 270% improvement from the raw LLM baseline on PopQA(Mallen et al., [2023](https://arxiv.org/html/2506.00054v1#bib.bib43)) and over 200% on ARC-Challenge(Clark et al., [2018](https://arxiv.org/html/2506.00054v1#bib.bib13)), illustrating the effectiveness of deep context integration for enhancing short-form factual recall. FiD-Light,', 'https://www.databricks.com/blog/long-context-rag-performance-llms': 'In this study, we benchmarked all LLMs on 4 curated RAG datasets that were formatted for both retrieval and generation. These included Databricks DocsQA and [FinanceBench](https://arxiv.org/abs/2311.11944), which represent industry use cases and Natural Questions (NQ) and HotPotQA, which represent more academic settings . Below are the dataset details: [...] **Saturation point:** as can be observed in the table, each dataset’s retrieval recall score saturates at a different context length. For the NQ dataset, it saturates early at 8k context length, whereas DocsQA, HotpotQA and FinanceBench datasets saturate at 96k and 128k context length, respectively. These results demonstrate that with a simple retrieval approach, there is additional relevant information available to the generation model all the way up to 96k or 128k tokens.\\xa0**Hence, the [...] The FinanceBench dataset is another use case specific benchmark that consists of\\xa0 longer documents, namely SEC 10k filings.\\xa0 In order to correctly answer the questions in the benchmark, the model needs a larger context length to capture relevant information from the corpus. This is likely the reason that, compared to other benchmarks, the recall for FinanceBench is low for small context sizes (Table 1). As a result, most models’ performance saturates at a longer context length than that of', 'https://aiagentslist.com/blog/is-rag-still-relevant-with-million-tokens-llms': '*   **Accuracy can suffer:** Pushing models to their maximum context limits can sometimes lead to [less reliable or lower-quality answers](https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows). [...] Large Language Models (LLMs) are evolving at breakneck speed. Models can now ‘read’ the equivalent of entire novels in one go, boasting massive context windows capable of handling [one million tokens or even more](https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows). This incredible leap raises a critical question for developers and businesses: Is Retrieval-Augmented Generation (RAG) still necessary? [...] *   **Performance Bottlenecks:** As discussed, models can suffer from [information overload](https://datasciencedojo.com/blog/the-llm-context-window-paradox/), leading to degraded accuracy and the “lost in the middle” issue where information is simply ignored.'}}}\n",
      "--  {'ask_question': {'messages': [AIMessage(content=\"That's excellent background information, thank you. My next question is:  Beyond simple accuracy metrics like exact match or F1-score, what qualitative differences have you observed in the *type* of hallucinations produced by RAG systems with and without million-plus token context windows?  Are certain types of hallucinations more or less prevalent with larger context windows?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, name='Dr. Anya Sharma', id='run--d86d3a07-a750-4346-a72b-0b98abc0c0ce-0', usage_metadata={'input_tokens': 632, 'output_tokens': 72, 'total_tokens': 704, 'input_token_details': {'cache_read': 0}})]}}\n",
      "--  {'answer_question': {'messages': [AIMessage(content=\"While precise qualitative differences in hallucination *types* between RAG systems using smaller versus million-plus token context windows aren't extensively documented in a single, unified study, available research suggests a shift in the nature of hallucinations.  With smaller context windows, hallucinations often manifest as factual omissions or fabrications directly related to missing information.<sup>1, 2</sup>  In contrast, larger context windows, while reducing these types of hallucinations, may lead to a different kind of error:  the model might create plausible-sounding but ultimately false connections between disparate pieces of information present within the extensive context.<sup>3, 4</sup> This can result in coherent but inaccurate summaries or inferences.<sup>5</sup>  Essentially, the type of hallucination shifts from simple factual errors to more sophisticated, but still incorrect, interpretations of the provided context.  Further research is needed to fully characterize these qualitative differences and their dependence on model architecture and training data.\\n\\nCitations:\\n\\n[1]: https://arxiv.org/html/2412.14905v1\\n[2]: https://arxiv.org/html/2503.00353v1\\n[3]: https://medium.com/@adnanmasood/long-context-windows-in-large-language-models-applications-in-comprehension-and-code-03bf4027066f\\n[4]: https://aws.amazon.com/blogs/machine-learning/detect-hallucinations-for-rag-based-systems/\\n[5]: https://customgpt.ai/long-context-windows-vs-rag/\", additional_kwargs={}, response_metadata={}, name='Subject Matter Expert')], 'references': {'https://arxiv.org/html/2503.00353v1': '| Noise Critical | Long Context + Noise Ratio >>> 97% | Extra Element | +355.82% | Occurs in long contexts | Smaller more affected | Hallucination surges with high noise at critical context length |\\n| Position-sensitive | Context Length <<< 16K + Reverse | Missing Element | +59.64% | Pronounced in small / medium contexts | Smaller more affected | Irrelevant info at the beginning interferes with understanding | [...] The noise critical pattern occurs under conditions of large Context sizes (approaching the maximum Context window) combined with extremely high noise ratios (Noise Ratio >>> 97%), where the model’s hallucination is significantly amplified, leading to a 355.82% increase in the probability of generating false elements. This situation is predominantly observed under the RAG Half and Full-Length setting. The critical value is also calculated by DecisionTreeRegressor and verified through statistical [...] Figure[11](https://arxiv.org/html/2503.00353v1#S5.F11 \"Figure 11 ‣ 5.2.2. Error Type under Perfect Retrieval ‣ 5.2. Error Attribution and Typical Patterns in RAG ‣ 5. Results and Discussion ‣ U-NIAH: Unified RAG and LLM Evaluation for Long Context Needle-In-A-Haystack\") illustrates the variation of omission and hallucination errors with respect to context length under three different settings: TopK, Half-Length, and Full-Length. The figure clearly demonstrates that as the context length', 'https://customgpt.ai/long-context-windows-vs-rag/': 'called co-pilot) is a kind of RAG system where it will take your prompt and use it to do a web search so that it can have more up to date info, better insight based on external sources, increased accuracy, and reduce hallucinations. But with the release of Google Gemini Pro 1.5 with its 1 million token context window, it begs the question, Did Google just kill RAG? [...] issue with their new model, showing that Gemini has near perfect recall across the entire 1 million tokens. Heck, they have actually [shown near perfect knowledge retrieval](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf) at up to 10 million tokens! [...] ![Image 23](blob:http://localhost/e2d483a2e3610f562762393b193297be)\\n\\n![Image 25](blob:http://localhost/e2d483a2e3610f562762393b193297be)\\n\\n![Image 27](blob:http://localhost/e2d483a2e3610f562762393b193297be)\\n\\n![Image 29](blob:http://localhost/2dd5f5a34d9e5d824b71e64602aac65b)\\n\\n![Image 31](blob:http://localhost/e2d483a2e3610f562762393b193297be)\\n\\n![Image 33](blob:http://localhost/e2d483a2e3610f562762393b193297be)\\n\\n![Image 35](blob:http://localhost/e2d483a2e3610f562762393b193297be)', 'https://medium.com/@adnanmasood/long-context-windows-in-large-language-models-applications-in-comprehension-and-code-03bf4027066f': '*   **Retrieval-Augmented Generation (RAG):** A paradigm where an LLM is supplemented with a retrieval step that fetches relevant text (e.g., from a database or the web) which is then provided as additional context to the model. Helps keep the model’s knowledge updated and focused without expanding context window arbitrarily.\\n*   **Hallucination:** In LLM context, when the model fabricates information that is not present or factual, typically confidently stating incorrect or non-existent facts. [...] *   **Hallucination changes:** There’s a subtle shift in how hallucinations manifest. A model with not enough context might hallucinate missing info. A model with a lot of context _might instead hallucinate connections_ between pieces of content that aren’t truly connected, just because it tries to find a pattern. It might generate a very convincing but entirely fictional summary that “sounds right” because it had so much detail to remix. In other words, long context doesn’t eliminate [...] *   **Pros, Cons, and Trade-offs:** Large context windows unlock powerful use cases — e.g. a single query can scan entire libraries or code repositories — and reduce the need to chop up input (which can miss cross-span dependencies). They also mitigate some instances of LLM “hallucinations” by providing all relevant information in-context . However, long context comes at a cost: **latency and expense** per request grow substantially with more tokens , and models may exhibit **recency bias**', 'https://aws.amazon.com/blogs/machine-learning/detect-hallucinations-for-rag-based-systems/': 'Solution overview\\n-----------------\\n\\nHallucinations can be categorized into three types, as illustrated in the following graphic.\\n\\n![Image 13](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/hallucination_image.jpg) [...] Approach 1: LLM-based hallucination detection\\n---------------------------------------------\\n\\nWe can use an LLM to classify the responses from our RAG system into context-conflicting hallucinations and facts. The aim is to identify which responses are based on the context or whether they contain hallucinations.\\n\\nThis approach consists of the following steps: [...] As RAG systems continue to evolve and play an increasingly important role in AI applications, the ability to detect and prevent hallucinations remains crucial. Through our exploration of four different approaches—LLM prompt-based detection, semantic similarity detection, BERT stochastic checking, and token similarity detection—we’ve demonstrated various methods to address this challenge. Although each approach has its strengths and trade-offs in terms of accuracy, precision, recall, and cost,', 'https://arxiv.org/html/2412.14905v1': 'Report issue for preceding element\\n\\nFigure 2:  Existing PCE approaches face two types of in-context hallucination issues when applied to RAG: (1) Fact fabrication. LLM generates fabricated answers that are inconsistent with the contextual facts. (2) Fact omission. The absence of required information in certain windows disproportionately influence the aggregation function, leading to disregard critical information in other windows. \\n\\nReport issue for preceding element [...] to answer when contexts are not related to questions; (2) for fact omission, we propose the information-calibrated aggregation which prioritizes context windows with higher information increment from their contexts. The experimental results on nine RAG tasks demonstrate that DePaC significantly alleviates the two types of in-context hallucination and consistently achieves better performances on these tasks. [...] Previous work(Weng, [2024](https://arxiv.org/html/2412.14905v1#bib.bib36)) categorizes hallucination into two types: (1) extrinsic hallucination, where the output of LLM is not grounded by the pre-training dataset or external world knowledge. (2) in-context hallucination, where the output of the model is inconsistent with the source content in context. In this work we focus on two types of in-context hallucination: (1) fact fabrication, where LLMs present claims that are not supported by the'}}}\n",
      "--  {'ask_question': {'messages': [AIMessage(content=\"That's a very insightful distinction. My next question is:  What are the computational and infrastructural challenges associated with deploying and running RAG systems that leverage million-plus token context windows, and what are some of the strategies being employed to mitigate these challenges?  I'm particularly interested in the trade-offs between accuracy gains and resource consumption.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, name='Dr. Anya Sharma', id='run--44b58b12-a246-4761-81cf-e0df7ab2ea5f-0', usage_metadata={'input_tokens': 1045, 'output_tokens': 71, 'total_tokens': 1116, 'input_token_details': {'cache_read': 0}})]}}\n",
      "--  {'answer_question': {'messages': [AIMessage(content='Deploying RAG systems with million-plus token context windows presents substantial computational and infrastructural challenges.  The sheer volume of data necessitates powerful hardware, including high-memory GPUs and specialized processors, leading to increased costs.<sup>1, 2</sup>  Furthermore, efficient data indexing and retrieval mechanisms are crucial for performance, requiring sophisticated infrastructure and potentially specialized databases.<sup>3</sup> Strategies to mitigate these challenges include techniques like query decomposition, which breaks down complex queries into smaller, manageable parts.<sup>4</sup>  Another approach is to employ efficient vector databases and retrieval methods to quickly locate relevant information within the massive context.<sup>5</sup>  Finally, careful consideration of the trade-offs between accuracy gains and resource consumption is essential;  optimizing retrieval strategies and using smaller context windows when appropriate can significantly reduce computational demands without sacrificing too much accuracy.<sup>6</sup>\\n\\nCitations:\\n\\n[1]: https://aiagentslist.com/blog/is-rag-still-relevant-with-million-tokens-llms\\n[2]: https://blog.dataiku.com/is-rag-obsolete\\n[3]: https://cohere.com/blog/rag-is-here-to-stay\\n[4]: https://fabrity.com/blog/will-large-context-windows-kill-rag-pipelines/\\n[5]: https://thenewstack.io/do-enormous-llm-context-windows-spell-the-end-of-rag/\\n[6]: https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows', additional_kwargs={}, response_metadata={}, name='Subject Matter Expert')], 'references': {'https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows': 'While an extended context window offers the advantage of processing more data at once, it introduces challenges related to model performance, accuracy, and efficiency. Processing millions of tokens demands substantial computational resources, leading to increased latency and higher operational costs. As context length grows, models can experience difficulties in maintaining attention and relevance across the entire input, potentially impacting the quality of AI outputs. On top this topic, [...] Another important consideration is that large and highly variable context windows can drive significant fluctuations in resource consumption. This places greater emphasis on intelligently balancing incoming requests to match available compute capacity. Advanced, adaptive load balancing solutions help distribute these large queries across multiple clusters or regions, mitigating bottlenecks and maintaining overall performance in complex AI deployments, even if they don’t directly reduce', 'https://thenewstack.io/do-enormous-llm-context-windows-spell-the-end-of-rag/': 'Expanded contextual windows in LLMs may provide a model with deeper insights, but it also brings challenges such as higher computational costs and efficiency. RAG tackles these challenges by selectively retrieving only the most relevant information, which helps optimize performance and accuracy.\\n\\n### Complex RAG Will Persist [...] State-of-the-art LLMs can process millions of tokens simultaneously, but the complexity and constant [evolution of data structures](https://roadmap.sh/ai-data-scientist) make it challenging for LLMs to manage heterogeneous enterprise data effectively. RAG addresses these challenges, although retrieval accuracy remains a major bottleneck for end-to-end performance. Whether it’s the large context window of LLMs or RAG, the goal is to make the best use of big data and ensure the high efficiency of [...] ### Performance Beyond Context Length\\n\\nWhile expanding the contextual window in LLMs to include millions of tokens looks promising, the practical implementations are still questionable due to several factors including time, efficiency and cost.', 'https://fabrity.com/blog/will-large-context-windows-kill-rag-pipelines/': 'trained to effectively utilize this expanded context, which presents its own set of challenges in terms of computational resources and training methodologies.', 'https://blog.dataiku.com/is-rag-obsolete': 'That said, It’s important to note that although long-context might be more sensitive to position bias, this problem can also impact RAG pipelines. A technique called [reordering can help mitigate this effect](https://python.langchain.com/docs/how_to/long_context_reorder/) by reorganizing the retrieved chunks to strategically place the most relevant information at the “top and tail” of the provided context. For example, in a RAG system with reordering, the retriever might initially select', 'https://aiagentslist.com/blog/is-rag-still-relevant-with-million-tokens-llms': '*   **Accuracy can suffer:** Pushing models to their maximum context limits can sometimes lead to [less reliable or lower-quality answers](https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows). [...] *   **RAG connects to live data:** It can query databases, APIs, or other systems to [fetch the absolute latest information in real-time](https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows). This is essential for applications needing up-to-the-minute accuracy. Think of a chatbot providing flight status updates – RAG pulls the latest data from the airline’s live system, while a large context window would only know the status from when the prompt was [...] While large context windows offer initial setup simplicity, RAG generally wins on cost-efficiency, output quality, security, and handling dynamic data – factors crucial for most enterprise applications.\\n\\nWhat are the Downsides of Relying Only on Large Contexts?\\n---------------------------------------------------------\\n\\nBeyond the comparison points, solely relying on massive context windows introduces several practical challenges:', 'https://cohere.com/blog/rag-is-here-to-stay': 'Before we dive in, it’s important to point out that for both large context windows and RAG systems, you’ll want to use a [model](https://cohere.com/blog/command-r-plus-microsoft-azure?ref=cohere-ai.ghost.io) that has been heavily trained to handle and interpret results from data retrieval. If not, when exposed to an excessive amount of documents, a model may saturate, meaning its performance plateaus or declines, and it may even start to produce confusing or irrelevant outputs. Proper training [...] There are on-going explorations to carry out long-context without decreasing performance using methods like caching. In the context of LLMs, caching refers to the process of storing and reusing previously generated responses or retrieved context from an LLM to improve efficiency and reduce latency. Simply put, it reduces the computational load. This is most apparent in use cases with many repetitive and overlapping queries. [...] The challenge comes with scale and usability, when caching longer and more dynamic data, context windows may not be a viable option. For example, if you want to cache one million 1-2k dimensional vectors, that\\'s an enormous amount of data. This means that you\\'ll need to keep this data in \"hot storage,\" which is a type of storage that allows for quick access, particularly important for tasks that require rapid data access, such as real-time data processing or high-frequency trading. This will'}}}\n",
      "--  {'ask_question': {'messages': [AIMessage(content='Thank you so much for your help!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, name='Dr. Anya Sharma', id='run--3c8e3ae2-ad62-4d04-b593-7d4ebf5d579e-0', usage_metadata={'input_tokens': 1462, 'output_tokens': 9, 'total_tokens': 1471, 'input_token_details': {'cache_read': 0}})]}}\n",
      "--  {'answer_question': {'messages': [AIMessage(content=\"You're most welcome! I'm glad I could assist you with your Wikipedia article.  Please let me know if you have any further questions.\\n\\nCitations:\\n\\n\", additional_kwargs={}, response_metadata={}, name='Subject Matter Expert')], 'references': {}}}\n"
     ]
    }
   ],
   "source": [
    "final_step = None\n",
    "initial_state = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": [AIMessage(\n",
    "        content=f\"So you said you were writing an article on {example_topic}?\",\n",
    "        name=\"Subject Matter Expert\"\n",
    "    )]\n",
    "}\n",
    "\n",
    "async for step in interview_graph.astream(initial_state):\n",
    "    name=next(iter(step))\n",
    "    # print(step)\n",
    "    print(\"-- \", step)\n",
    "    final_step = step\n",
    "    \n",
    "\n",
    "final_state = next(iter(final_step.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c92ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'answer_question'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42073e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
